//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-19324607
// Cuda compilation tools, release 7.0, V7.0.27
// Based on LLVM 3.4svn
//

.version 4.2
.target sm_30
.address_size 64

	// .globl	_Z6sdotvvPKdS0_i
.extern .func __assertfail
(
	.param .b64 __assertfail_param_0,
	.param .b64 __assertfail_param_1,
	.param .b32 __assertfail_param_2,
	.param .b64 __assertfail_param_3,
	.param .b64 __assertfail_param_4
)
;
.global .align 1 .b8 __T20[39] = {118, 111, 105, 100, 32, 115, 117, 109, 40, 105, 110, 116, 32, 42, 44, 32, 105, 110, 116, 32, 42, 44, 32, 108, 111, 110, 103, 44, 32, 105, 110, 116, 44, 32, 105, 110, 116, 41, 0};
.global .align 1 .b8 __T21[75] = {118, 111, 105, 100, 32, 105, 110, 116, 65, 114, 114, 97, 121, 83, 117, 109, 40, 99, 111, 110, 115, 116, 32, 108, 111, 110, 103, 32, 42, 44, 32, 99, 111, 110, 115, 116, 32, 105, 110, 116, 32, 42, 44, 32, 108, 111, 110, 103, 32, 42, 44, 32, 105, 110, 116, 32, 42, 44, 32, 108, 111, 110, 103, 44, 32, 105, 110, 116, 44, 32, 105, 110, 116, 41, 0};
.global .align 1 .b8 __T22[85] = {118, 111, 105, 100, 32, 68, 97, 116, 97, 80, 111, 105, 110, 116, 82, 101, 100, 117, 99, 101, 40, 99, 111, 110, 115, 116, 32, 108, 111, 110, 103, 32, 42, 44, 32, 99, 111, 110, 115, 116, 32, 100, 111, 117, 98, 108, 101, 32, 42, 44, 32, 108, 111, 110, 103, 32, 42, 44, 32, 100, 111, 117, 98, 108, 101, 32, 42, 44, 32, 108, 111, 110, 103, 44, 32, 105, 110, 116, 44, 32, 105, 110, 116, 41, 0};
.global .align 1 .b8 $str[31] = {106, 117, 109, 112, 32, 61, 61, 32, 98, 108, 111, 99, 107, 68, 105, 109, 46, 120, 32, 42, 32, 103, 114, 105, 100, 68, 105, 109, 46, 120, 0};
.global .align 1 .b8 $str1[19] = {116, 101, 115, 116, 67, 85, 68, 65, 75, 101, 114, 110, 101, 108, 115, 46, 99, 117, 0};

.visible .func  (.param .b64 func_retval0) _Z6sdotvvPKdS0_i(
	.param .b64 _Z6sdotvvPKdS0_i_param_0,
	.param .b64 _Z6sdotvvPKdS0_i_param_1,
	.param .b32 _Z6sdotvvPKdS0_i_param_2
)
{
	.reg .pred 	%p<3>;
	.reg .s32 	%r<6>;
	.reg .f64 	%fd<11>;
	.reg .s64 	%rd<9>;


	ld.param.u64 	%rd7, [_Z6sdotvvPKdS0_i_param_0];
	ld.param.u64 	%rd8, [_Z6sdotvvPKdS0_i_param_1];
	ld.param.u32 	%r3, [_Z6sdotvvPKdS0_i_param_2];
	mov.f64 	%fd9, 0d0000000000000000;
	mov.f64 	%fd10, %fd9;
	mov.u32 	%r5, 0;
	setp.lt.s32	%p1, %r3, 1;
	@%p1 bra 	BB0_2;

BB0_1:
	ld.f64 	%fd6, [%rd8];
	ld.f64 	%fd7, [%rd7];
	fma.rn.f64 	%fd10, %fd7, %fd6, %fd10;
	add.s64 	%rd8, %rd8, 8;
	add.s64 	%rd7, %rd7, 8;
	add.s32 	%r5, %r5, 1;
	setp.lt.s32	%p2, %r5, %r3;
	mov.f64 	%fd9, %fd10;
	@%p2 bra 	BB0_1;

BB0_2:
	st.param.f64	[func_retval0+0], %fd9;
	ret;
}

	// .globl	_Z6dmulvsPdPKddi
.visible .func _Z6dmulvsPdPKddi(
	.param .b64 _Z6dmulvsPdPKddi_param_0,
	.param .b64 _Z6dmulvsPdPKddi_param_1,
	.param .b64 _Z6dmulvsPdPKddi_param_2,
	.param .b32 _Z6dmulvsPdPKddi_param_3
)
{
	.reg .pred 	%p<3>;
	.reg .s32 	%r<6>;
	.reg .f64 	%fd<4>;
	.reg .s64 	%rd<9>;


	ld.param.u64 	%rd8, [_Z6dmulvsPdPKddi_param_0];
	ld.param.u64 	%rd7, [_Z6dmulvsPdPKddi_param_1];
	ld.param.f64 	%fd1, [_Z6dmulvsPdPKddi_param_2];
	ld.param.u32 	%r3, [_Z6dmulvsPdPKddi_param_3];
	mov.u32 	%r5, 0;
	setp.lt.s32	%p1, %r3, 1;
	@%p1 bra 	BB1_2;

BB1_1:
	ld.f64 	%fd2, [%rd7];
	mul.f64 	%fd3, %fd2, %fd1;
	st.f64 	[%rd8], %fd3;
	add.s64 	%rd8, %rd8, 8;
	add.s64 	%rd7, %rd7, 8;
	add.s32 	%r5, %r5, 1;
	setp.lt.s32	%p2, %r5, %r3;
	@%p2 bra 	BB1_1;

BB1_2:
	ret;
}

	// .globl	_Z3mapPdPKddS1_i
.visible .func _Z3mapPdPKddS1_i(
	.param .b64 _Z3mapPdPKddS1_i_param_0,
	.param .b64 _Z3mapPdPKddS1_i_param_1,
	.param .b64 _Z3mapPdPKddS1_i_param_2,
	.param .b64 _Z3mapPdPKddS1_i_param_3,
	.param .b32 _Z3mapPdPKddS1_i_param_4
)
{
	.reg .pred 	%p<9>;
	.reg .f32 	%f<3>;
	.reg .s32 	%r<24>;
	.reg .f64 	%fd<64>;
	.reg .s64 	%rd<17>;


	ld.param.u64 	%rd16, [_Z3mapPdPKddS1_i_param_0];
	ld.param.u64 	%rd10, [_Z3mapPdPKddS1_i_param_1];
	ld.param.f64 	%fd13, [_Z3mapPdPKddS1_i_param_2];
	ld.param.u64 	%rd12, [_Z3mapPdPKddS1_i_param_3];
	ld.param.u32 	%r10, [_Z3mapPdPKddS1_i_param_4];
	mov.f64 	%fd60, 0d0000000000000000;
	mov.f64 	%fd61, %fd60;
	mov.u32 	%r21, 0;
	setp.lt.s32	%p1, %r10, 1;
	@%p1 bra 	BB2_3;

	mov.u64 	%rd15, %rd10;

BB2_2:
	mov.u64 	%rd2, %rd15;
	ld.f64 	%fd16, [%rd2];
	ld.f64 	%fd17, [%rd12];
	fma.rn.f64 	%fd61, %fd17, %fd16, %fd61;
	add.s64 	%rd3, %rd2, 8;
	add.s64 	%rd12, %rd12, 8;
	add.s32 	%r21, %r21, 1;
	setp.lt.s32	%p2, %r21, %r10;
	mov.f64 	%fd60, %fd61;
	mov.u64 	%rd15, %rd3;
	@%p2 bra 	BB2_2;

BB2_3:
	mul.f64 	%fd4, %fd60, %fd13;
	neg.f64 	%fd5, %fd4;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r3}, %fd5;
	}
	mov.b32 	 %f1, %r3;
	abs.ftz.f32 	%f2, %f1;
	setp.lt.ftz.f32	%p3, %f2, 0f40874911;
	@%p3 bra 	BB2_5;
	bra.uni 	BB2_4;

BB2_5:
	mov.f64 	%fd21, 0d3FF71547652B82FE;
	mul.rn.f64 	%fd22, %fd5, %fd21;
	mov.f64 	%fd23, 0d4338000000000000;
	add.rn.f64 	%fd24, %fd22, %fd23;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r4, %temp}, %fd24;
	}
	mov.f64 	%fd25, 0dC338000000000000;
	add.rn.f64 	%fd26, %fd24, %fd25;
	mov.f64 	%fd27, 0dBFE62E42FEFA39EF;
	fma.rn.f64 	%fd28, %fd26, %fd27, %fd5;
	mov.f64 	%fd29, 0dBC7ABC9E3B39803F;
	fma.rn.f64 	%fd30, %fd26, %fd29, %fd28;
	mov.f64 	%fd31, 0d3E928AF3FCA213EA;
	mov.f64 	%fd32, 0d3E5ADE1569CE2BDF;
	fma.rn.f64 	%fd33, %fd32, %fd30, %fd31;
	mov.f64 	%fd34, 0d3EC71DEE62401315;
	fma.rn.f64 	%fd35, %fd33, %fd30, %fd34;
	mov.f64 	%fd36, 0d3EFA01997C89EB71;
	fma.rn.f64 	%fd37, %fd35, %fd30, %fd36;
	mov.f64 	%fd38, 0d3F2A01A014761F65;
	fma.rn.f64 	%fd39, %fd37, %fd30, %fd38;
	mov.f64 	%fd40, 0d3F56C16C1852B7AF;
	fma.rn.f64 	%fd41, %fd39, %fd30, %fd40;
	mov.f64 	%fd42, 0d3F81111111122322;
	fma.rn.f64 	%fd43, %fd41, %fd30, %fd42;
	mov.f64 	%fd44, 0d3FA55555555502A1;
	fma.rn.f64 	%fd45, %fd43, %fd30, %fd44;
	mov.f64 	%fd46, 0d3FC5555555555511;
	fma.rn.f64 	%fd47, %fd45, %fd30, %fd46;
	mov.f64 	%fd48, 0d3FE000000000000B;
	fma.rn.f64 	%fd49, %fd47, %fd30, %fd48;
	mov.f64 	%fd50, 0d3FF0000000000000;
	fma.rn.f64 	%fd51, %fd49, %fd30, %fd50;
	fma.rn.f64 	%fd62, %fd51, %fd30, %fd50;
	abs.s32 	%r12, %r4;
	setp.lt.s32	%p6, %r12, 1023;
	@%p6 bra 	BB2_7;
	bra.uni 	BB2_6;

BB2_7:
	shl.b32 	%r18, %r4, 20;
	add.s32 	%r22, %r18, 1072693248;
	bra.uni 	BB2_8;

BB2_4:
	setp.lt.s32	%p4, %r3, 0;
	selp.f64	%fd18, 0d0000000000000000, 0d7FF0000000000000, %p4;
	abs.f64 	%fd19, %fd5;
	setp.gtu.f64	%p5, %fd19, 0d7FF0000000000000;
	sub.f64 	%fd20, %fd5, %fd4;
	selp.f64	%fd63, %fd20, %fd18, %p5;
	bra.uni 	BB2_9;

BB2_6:
	add.s32 	%r13, %r4, 2046;
	shl.b32 	%r14, %r13, 19;
	and.b32  	%r15, %r14, -1048576;
	shl.b32 	%r16, %r13, 20;
	sub.s32 	%r22, %r16, %r15;
	mov.u32 	%r17, 0;
	mov.b64 	%fd52, {%r17, %r15};
	mul.f64 	%fd62, %fd62, %fd52;

BB2_8:
	mov.u32 	%r19, 0;
	mov.b64 	%fd53, {%r19, %r22};
	mul.f64 	%fd63, %fd62, %fd53;

BB2_9:
	add.f64 	%fd54, %fd63, 0d3FF0000000000000;
	rcp.rn.f64 	%fd55, %fd54;
	add.f64 	%fd56, %fd55, 0dBFF0000000000000;
	mul.f64 	%fd12, %fd56, %fd13;
	mov.u32 	%r23, 0;
	@%p1 bra 	BB2_12;

	mov.u64 	%rd14, %rd10;

BB2_11:
	ld.f64 	%fd57, [%rd14];
	mul.f64 	%fd58, %fd12, %fd57;
	st.f64 	[%rd16], %fd58;
	add.s64 	%rd16, %rd16, 8;
	add.s64 	%rd14, %rd14, 8;
	add.s32 	%r23, %r23, 1;
	setp.lt.s32	%p8, %r23, %r10;
	@%p8 bra 	BB2_11;

BB2_12:
	ret;
}

	// .globl	_Z15atomicAddDoublePdd
.visible .func  (.param .b64 func_retval0) _Z15atomicAddDoublePdd(
	.param .b64 _Z15atomicAddDoublePdd_param_0,
	.param .b64 _Z15atomicAddDoublePdd_param_1
)
{
	.reg .pred 	%p<2>;
	.reg .f64 	%fd<4>;
	.reg .s64 	%rd<7>;


	ld.param.u64 	%rd1, [_Z15atomicAddDoublePdd_param_0];
	ld.param.f64 	%fd2, [_Z15atomicAddDoublePdd_param_1];
	ld.u64 	%rd6, [%rd1];

BB3_1:
	mov.u64 	%rd3, %rd6;
	mov.b64 	 %fd1, %rd3;
	add.f64 	%fd3, %fd1, %fd2;
	mov.b64 	 %rd5, %fd3;
	atom.cas.b64 	%rd6, [%rd1], %rd3, %rd5;
	setp.ne.s64	%p1, %rd3, %rd6;
	@%p1 bra 	BB3_1;

	st.param.f64	[func_retval0+0], %fd1;
	ret;
}

	// .globl	_Z13__shfl_doubledi
.visible .func  (.param .b64 func_retval0) _Z13__shfl_doubledi(
	.param .b64 _Z13__shfl_doubledi_param_0,
	.param .b32 _Z13__shfl_doubledi_param_1
)
{
	.reg .s32 	%r<13>;
	.reg .f64 	%fd<3>;


	ld.param.f64 	%fd1, [_Z13__shfl_doubledi_param_0];
	ld.param.u32 	%r5, [_Z13__shfl_doubledi_param_1];
	// inline asm
	mov.b64 {%r1,%r2}, %fd1;
	// inline asm
	mov.u32 	%r10, 31;
	// inline asm
	shfl.idx.b32 %r3, %r1, %r5, %r10;
	// inline asm
	// inline asm
	shfl.idx.b32 %r7, %r2, %r5, %r10;
	// inline asm
	// inline asm
	mov.b64 %fd2, {%r3,%r7};
	// inline asm
	st.param.f64	[func_retval0+0], %fd2;
	ret;
}

	// .globl	_Z13warpReduceSumd
.visible .func  (.param .b64 func_retval0) _Z13warpReduceSumd(
	.param .b64 _Z13warpReduceSumd_param_0
)
{
	.reg .s32 	%r<90>;
	.reg .f64 	%fd<12>;


	ld.param.f64 	%fd1, [_Z13warpReduceSumd_param_0];
	mov.u32 	%r61, %ctaid.x;
	mov.u32 	%r62, %ntid.x;
	mov.u32 	%r63, %tid.x;
	mad.lo.s32 	%r64, %r62, %r61, %r63;
	add.s32 	%r65, %r64, 16;
	shr.s32 	%r66, %r65, 31;
	shr.u32 	%r67, %r66, 27;
	add.s32 	%r68, %r65, %r67;
	and.b32  	%r69, %r68, -32;
	sub.s32 	%r5, %r65, %r69;
	// inline asm
	mov.b64 {%r1,%r2}, %fd1;
	// inline asm
	mov.u32 	%r58, 31;
	// inline asm
	shfl.idx.b32 %r3, %r1, %r5, %r58;
	// inline asm
	// inline asm
	shfl.idx.b32 %r7, %r2, %r5, %r58;
	// inline asm
	// inline asm
	mov.b64 %fd2, {%r3,%r7};
	// inline asm
	add.f64 	%fd3, %fd2, %fd1;
	add.s32 	%r70, %r64, 8;
	shr.s32 	%r71, %r70, 31;
	shr.u32 	%r72, %r71, 27;
	add.s32 	%r73, %r70, %r72;
	and.b32  	%r74, %r73, -32;
	sub.s32 	%r17, %r70, %r74;
	// inline asm
	mov.b64 {%r13,%r14}, %fd3;
	// inline asm
	// inline asm
	shfl.idx.b32 %r15, %r13, %r17, %r58;
	// inline asm
	// inline asm
	shfl.idx.b32 %r19, %r14, %r17, %r58;
	// inline asm
	// inline asm
	mov.b64 %fd4, {%r15,%r19};
	// inline asm
	add.f64 	%fd5, %fd3, %fd4;
	add.s32 	%r75, %r64, 4;
	shr.s32 	%r76, %r75, 31;
	shr.u32 	%r77, %r76, 27;
	add.s32 	%r78, %r75, %r77;
	and.b32  	%r79, %r78, -32;
	sub.s32 	%r29, %r75, %r79;
	// inline asm
	mov.b64 {%r25,%r26}, %fd5;
	// inline asm
	// inline asm
	shfl.idx.b32 %r27, %r25, %r29, %r58;
	// inline asm
	// inline asm
	shfl.idx.b32 %r31, %r26, %r29, %r58;
	// inline asm
	// inline asm
	mov.b64 %fd6, {%r27,%r31};
	// inline asm
	add.f64 	%fd7, %fd5, %fd6;
	add.s32 	%r80, %r64, 2;
	shr.s32 	%r81, %r80, 31;
	shr.u32 	%r82, %r81, 27;
	add.s32 	%r83, %r80, %r82;
	and.b32  	%r84, %r83, -32;
	sub.s32 	%r41, %r80, %r84;
	// inline asm
	mov.b64 {%r37,%r38}, %fd7;
	// inline asm
	// inline asm
	shfl.idx.b32 %r39, %r37, %r41, %r58;
	// inline asm
	// inline asm
	shfl.idx.b32 %r43, %r38, %r41, %r58;
	// inline asm
	// inline asm
	mov.b64 %fd8, {%r39,%r43};
	// inline asm
	add.f64 	%fd9, %fd7, %fd8;
	add.s32 	%r85, %r64, 1;
	shr.s32 	%r86, %r85, 31;
	shr.u32 	%r87, %r86, 27;
	add.s32 	%r88, %r85, %r87;
	and.b32  	%r89, %r88, -32;
	sub.s32 	%r53, %r85, %r89;
	// inline asm
	mov.b64 {%r49,%r50}, %fd9;
	// inline asm
	// inline asm
	shfl.idx.b32 %r51, %r49, %r53, %r58;
	// inline asm
	// inline asm
	shfl.idx.b32 %r55, %r50, %r53, %r58;
	// inline asm
	// inline asm
	mov.b64 %fd10, {%r51,%r55};
	// inline asm
	add.f64 	%fd11, %fd9, %fd10;
	st.param.f64	[func_retval0+0], %fd11;
	ret;
}

	// .globl	_Z14__shfl_double47double4i
.visible .func  (.param .align 16 .b8 func_retval0[32]) _Z14__shfl_double47double4i(
	.param .align 16 .b8 _Z14__shfl_double47double4i_param_0[32],
	.param .b32 _Z14__shfl_double47double4i_param_1
)
{
	.reg .s32 	%r<49>;
	.reg .f64 	%fd<9>;


	ld.param.f64 	%fd4, [_Z14__shfl_double47double4i_param_0+24];
	ld.param.f64 	%fd3, [_Z14__shfl_double47double4i_param_0+16];
	ld.param.f64 	%fd2, [_Z14__shfl_double47double4i_param_0+8];
	ld.param.f64 	%fd1, [_Z14__shfl_double47double4i_param_0];
	ld.param.u32 	%r11, [_Z14__shfl_double47double4i_param_1];
	// inline asm
	mov.b64 {%r1,%r2}, %fd1;
	// inline asm
	// inline asm
	mov.b64 {%r3,%r4}, %fd2;
	// inline asm
	// inline asm
	mov.b64 {%r5,%r6}, %fd3;
	// inline asm
	// inline asm
	mov.b64 {%r7,%r8}, %fd4;
	// inline asm
	mov.u32 	%r40, 31;
	// inline asm
	shfl.idx.b32 %r9, %r1, %r11, %r40;
	// inline asm
	// inline asm
	shfl.idx.b32 %r13, %r2, %r11, %r40;
	// inline asm
	// inline asm
	shfl.idx.b32 %r17, %r3, %r11, %r40;
	// inline asm
	// inline asm
	shfl.idx.b32 %r21, %r4, %r11, %r40;
	// inline asm
	// inline asm
	shfl.idx.b32 %r25, %r5, %r11, %r40;
	// inline asm
	// inline asm
	shfl.idx.b32 %r29, %r6, %r11, %r40;
	// inline asm
	// inline asm
	shfl.idx.b32 %r33, %r7, %r11, %r40;
	// inline asm
	// inline asm
	shfl.idx.b32 %r37, %r8, %r11, %r40;
	// inline asm
	// inline asm
	mov.b64 %fd5, {%r9,%r13};
	// inline asm
	// inline asm
	mov.b64 %fd6, {%r17,%r21};
	// inline asm
	// inline asm
	mov.b64 %fd7, {%r25,%r29};
	// inline asm
	// inline asm
	mov.b64 %fd8, {%r33,%r37};
	// inline asm
	st.param.f64	[func_retval0+0], %fd5;
	st.param.f64	[func_retval0+8], %fd6;
	st.param.f64	[func_retval0+16], %fd7;
	st.param.f64	[func_retval0+24], %fd8;
	ret;
}

	// .globl	_Z14warpReduceVSum7double4
.visible .func  (.param .align 16 .b8 func_retval0[32]) _Z14warpReduceVSum7double4(
	.param .align 16 .b8 _Z14warpReduceVSum7double4_param_0[32]
)
{
	.reg .s32 	%r<270>;
	.reg .f64 	%fd<45>;


	ld.param.f64 	%fd4, [_Z14warpReduceVSum7double4_param_0+24];
	ld.param.f64 	%fd3, [_Z14warpReduceVSum7double4_param_0+16];
	ld.param.f64 	%fd2, [_Z14warpReduceVSum7double4_param_0+8];
	ld.param.f64 	%fd1, [_Z14warpReduceVSum7double4_param_0];
	mov.u32 	%r241, %ctaid.x;
	mov.u32 	%r242, %ntid.x;
	mov.u32 	%r243, %tid.x;
	mad.lo.s32 	%r244, %r242, %r241, %r243;
	add.s32 	%r245, %r244, 16;
	shr.s32 	%r246, %r245, 31;
	shr.u32 	%r247, %r246, 27;
	add.s32 	%r248, %r245, %r247;
	and.b32  	%r249, %r248, -32;
	sub.s32 	%r11, %r245, %r249;
	// inline asm
	mov.b64 {%r1,%r2}, %fd1;
	// inline asm
	// inline asm
	mov.b64 {%r3,%r4}, %fd2;
	// inline asm
	// inline asm
	mov.b64 {%r5,%r6}, %fd3;
	// inline asm
	// inline asm
	mov.b64 {%r7,%r8}, %fd4;
	// inline asm
	mov.u32 	%r232, 31;
	// inline asm
	shfl.idx.b32 %r9, %r1, %r11, %r232;
	// inline asm
	// inline asm
	shfl.idx.b32 %r13, %r2, %r11, %r232;
	// inline asm
	// inline asm
	shfl.idx.b32 %r17, %r3, %r11, %r232;
	// inline asm
	// inline asm
	shfl.idx.b32 %r21, %r4, %r11, %r232;
	// inline asm
	// inline asm
	shfl.idx.b32 %r25, %r5, %r11, %r232;
	// inline asm
	// inline asm
	shfl.idx.b32 %r29, %r6, %r11, %r232;
	// inline asm
	// inline asm
	shfl.idx.b32 %r33, %r7, %r11, %r232;
	// inline asm
	// inline asm
	shfl.idx.b32 %r37, %r8, %r11, %r232;
	// inline asm
	// inline asm
	mov.b64 %fd5, {%r9,%r13};
	// inline asm
	// inline asm
	mov.b64 %fd6, {%r17,%r21};
	// inline asm
	// inline asm
	mov.b64 %fd7, {%r25,%r29};
	// inline asm
	// inline asm
	mov.b64 %fd8, {%r33,%r37};
	// inline asm
	add.f64 	%fd9, %fd1, %fd5;
	add.f64 	%fd10, %fd2, %fd6;
	add.f64 	%fd11, %fd3, %fd7;
	add.f64 	%fd12, %fd4, %fd8;
	add.s32 	%r250, %r244, 8;
	shr.s32 	%r251, %r250, 31;
	shr.u32 	%r252, %r251, 27;
	add.s32 	%r253, %r250, %r252;
	and.b32  	%r254, %r253, -32;
	sub.s32 	%r59, %r250, %r254;
	// inline asm
	mov.b64 {%r49,%r50}, %fd9;
	// inline asm
	// inline asm
	mov.b64 {%r51,%r52}, %fd10;
	// inline asm
	// inline asm
	mov.b64 {%r53,%r54}, %fd11;
	// inline asm
	// inline asm
	mov.b64 {%r55,%r56}, %fd12;
	// inline asm
	// inline asm
	shfl.idx.b32 %r57, %r49, %r59, %r232;
	// inline asm
	// inline asm
	shfl.idx.b32 %r61, %r50, %r59, %r232;
	// inline asm
	// inline asm
	shfl.idx.b32 %r65, %r51, %r59, %r232;
	// inline asm
	// inline asm
	shfl.idx.b32 %r69, %r52, %r59, %r232;
	// inline asm
	// inline asm
	shfl.idx.b32 %r73, %r53, %r59, %r232;
	// inline asm
	// inline asm
	shfl.idx.b32 %r77, %r54, %r59, %r232;
	// inline asm
	// inline asm
	shfl.idx.b32 %r81, %r55, %r59, %r232;
	// inline asm
	// inline asm
	shfl.idx.b32 %r85, %r56, %r59, %r232;
	// inline asm
	// inline asm
	mov.b64 %fd13, {%r57,%r61};
	// inline asm
	// inline asm
	mov.b64 %fd14, {%r65,%r69};
	// inline asm
	// inline asm
	mov.b64 %fd15, {%r73,%r77};
	// inline asm
	// inline asm
	mov.b64 %fd16, {%r81,%r85};
	// inline asm
	add.f64 	%fd17, %fd9, %fd13;
	add.f64 	%fd18, %fd10, %fd14;
	add.f64 	%fd19, %fd11, %fd15;
	add.f64 	%fd20, %fd12, %fd16;
	add.s32 	%r255, %r244, 4;
	shr.s32 	%r256, %r255, 31;
	shr.u32 	%r257, %r256, 27;
	add.s32 	%r258, %r255, %r257;
	and.b32  	%r259, %r258, -32;
	sub.s32 	%r107, %r255, %r259;
	// inline asm
	mov.b64 {%r97,%r98}, %fd17;
	// inline asm
	// inline asm
	mov.b64 {%r99,%r100}, %fd18;
	// inline asm
	// inline asm
	mov.b64 {%r101,%r102}, %fd19;
	// inline asm
	// inline asm
	mov.b64 {%r103,%r104}, %fd20;
	// inline asm
	// inline asm
	shfl.idx.b32 %r105, %r97, %r107, %r232;
	// inline asm
	// inline asm
	shfl.idx.b32 %r109, %r98, %r107, %r232;
	// inline asm
	// inline asm
	shfl.idx.b32 %r113, %r99, %r107, %r232;
	// inline asm
	// inline asm
	shfl.idx.b32 %r117, %r100, %r107, %r232;
	// inline asm
	// inline asm
	shfl.idx.b32 %r121, %r101, %r107, %r232;
	// inline asm
	// inline asm
	shfl.idx.b32 %r125, %r102, %r107, %r232;
	// inline asm
	// inline asm
	shfl.idx.b32 %r129, %r103, %r107, %r232;
	// inline asm
	// inline asm
	shfl.idx.b32 %r133, %r104, %r107, %r232;
	// inline asm
	// inline asm
	mov.b64 %fd21, {%r105,%r109};
	// inline asm
	// inline asm
	mov.b64 %fd22, {%r113,%r117};
	// inline asm
	// inline asm
	mov.b64 %fd23, {%r121,%r125};
	// inline asm
	// inline asm
	mov.b64 %fd24, {%r129,%r133};
	// inline asm
	add.f64 	%fd25, %fd17, %fd21;
	add.f64 	%fd26, %fd18, %fd22;
	add.f64 	%fd27, %fd19, %fd23;
	add.f64 	%fd28, %fd20, %fd24;
	add.s32 	%r260, %r244, 2;
	shr.s32 	%r261, %r260, 31;
	shr.u32 	%r262, %r261, 27;
	add.s32 	%r263, %r260, %r262;
	and.b32  	%r264, %r263, -32;
	sub.s32 	%r155, %r260, %r264;
	// inline asm
	mov.b64 {%r145,%r146}, %fd25;
	// inline asm
	// inline asm
	mov.b64 {%r147,%r148}, %fd26;
	// inline asm
	// inline asm
	mov.b64 {%r149,%r150}, %fd27;
	// inline asm
	// inline asm
	mov.b64 {%r151,%r152}, %fd28;
	// inline asm
	// inline asm
	shfl.idx.b32 %r153, %r145, %r155, %r232;
	// inline asm
	// inline asm
	shfl.idx.b32 %r157, %r146, %r155, %r232;
	// inline asm
	// inline asm
	shfl.idx.b32 %r161, %r147, %r155, %r232;
	// inline asm
	// inline asm
	shfl.idx.b32 %r165, %r148, %r155, %r232;
	// inline asm
	// inline asm
	shfl.idx.b32 %r169, %r149, %r155, %r232;
	// inline asm
	// inline asm
	shfl.idx.b32 %r173, %r150, %r155, %r232;
	// inline asm
	// inline asm
	shfl.idx.b32 %r177, %r151, %r155, %r232;
	// inline asm
	// inline asm
	shfl.idx.b32 %r181, %r152, %r155, %r232;
	// inline asm
	// inline asm
	mov.b64 %fd29, {%r153,%r157};
	// inline asm
	// inline asm
	mov.b64 %fd30, {%r161,%r165};
	// inline asm
	// inline asm
	mov.b64 %fd31, {%r169,%r173};
	// inline asm
	// inline asm
	mov.b64 %fd32, {%r177,%r181};
	// inline asm
	add.f64 	%fd33, %fd25, %fd29;
	add.f64 	%fd34, %fd26, %fd30;
	add.f64 	%fd35, %fd27, %fd31;
	add.f64 	%fd36, %fd28, %fd32;
	add.s32 	%r265, %r244, 1;
	shr.s32 	%r266, %r265, 31;
	shr.u32 	%r267, %r266, 27;
	add.s32 	%r268, %r265, %r267;
	and.b32  	%r269, %r268, -32;
	sub.s32 	%r203, %r265, %r269;
	// inline asm
	mov.b64 {%r193,%r194}, %fd33;
	// inline asm
	// inline asm
	mov.b64 {%r195,%r196}, %fd34;
	// inline asm
	// inline asm
	mov.b64 {%r197,%r198}, %fd35;
	// inline asm
	// inline asm
	mov.b64 {%r199,%r200}, %fd36;
	// inline asm
	// inline asm
	shfl.idx.b32 %r201, %r193, %r203, %r232;
	// inline asm
	// inline asm
	shfl.idx.b32 %r205, %r194, %r203, %r232;
	// inline asm
	// inline asm
	shfl.idx.b32 %r209, %r195, %r203, %r232;
	// inline asm
	// inline asm
	shfl.idx.b32 %r213, %r196, %r203, %r232;
	// inline asm
	// inline asm
	shfl.idx.b32 %r217, %r197, %r203, %r232;
	// inline asm
	// inline asm
	shfl.idx.b32 %r221, %r198, %r203, %r232;
	// inline asm
	// inline asm
	shfl.idx.b32 %r225, %r199, %r203, %r232;
	// inline asm
	// inline asm
	shfl.idx.b32 %r229, %r200, %r203, %r232;
	// inline asm
	// inline asm
	mov.b64 %fd37, {%r201,%r205};
	// inline asm
	// inline asm
	mov.b64 %fd38, {%r209,%r213};
	// inline asm
	// inline asm
	mov.b64 %fd39, {%r217,%r221};
	// inline asm
	// inline asm
	mov.b64 %fd40, {%r225,%r229};
	// inline asm
	add.f64 	%fd41, %fd33, %fd37;
	add.f64 	%fd42, %fd34, %fd38;
	add.f64 	%fd43, %fd35, %fd39;
	add.f64 	%fd44, %fd36, %fd40;
	st.param.f64	[func_retval0+0], %fd41;
	st.param.f64	[func_retval0+8], %fd42;
	st.param.f64	[func_retval0+16], %fd43;
	st.param.f64	[func_retval0+24], %fd44;
	ret;
}

	// .globl	_Z18deviceReduceKernelPKlPKdPdll
.visible .func  (.param .b64 func_retval0) _Z18deviceReduceKernelPKlPKdPdll(
	.param .b64 _Z18deviceReduceKernelPKlPKdPdll_param_0,
	.param .b64 _Z18deviceReduceKernelPKlPKdPdll_param_1,
	.param .b64 _Z18deviceReduceKernelPKlPKdPdll_param_2,
	.param .b64 _Z18deviceReduceKernelPKlPKdPdll_param_3,
	.param .b64 _Z18deviceReduceKernelPKlPKdPdll_param_4
)
{
	.reg .pred 	%p<5>;
	.reg .s32 	%r<94>;
	.reg .f64 	%fd<21>;
	.reg .s64 	%rd<25>;


	ld.param.u64 	%rd10, [_Z18deviceReduceKernelPKlPKdPdll_param_0];
	ld.param.u64 	%rd11, [_Z18deviceReduceKernelPKlPKdPdll_param_1];
	ld.param.u64 	%rd12, [_Z18deviceReduceKernelPKlPKdPdll_param_2];
	ld.param.u64 	%rd13, [_Z18deviceReduceKernelPKlPKdPdll_param_3];
	ld.param.u64 	%rd14, [_Z18deviceReduceKernelPKlPKdPdll_param_4];
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r2, %tid.x;
	mad.lo.s32 	%r3, %r1, %r4, %r2;
	cvt.u64.u32	%rd1, %r3;
	mov.f64 	%fd20, 0d0000000000000000;
	setp.ge.s64	%p1, %rd1, %rd14;
	@%p1 bra 	BB8_3;

	add.s64 	%rd2, %rd13, 16;
	mov.u32 	%r5, %nctaid.x;
	mul.lo.s32 	%r6, %r5, %r1;
	cvt.u64.u32	%rd3, %r6;
	mov.f64 	%fd20, 0d0000000000000000;
	mov.u64 	%rd23, %rd1;

BB8_2:
	mov.u64 	%rd4, %rd23;
	shl.b64 	%rd15, %rd4, 3;
	add.s64 	%rd16, %rd10, %rd15;
	ld.u64 	%rd17, [%rd16];
	shr.u64 	%rd18, %rd17, 3;
	add.s64 	%rd19, %rd2, %rd18;
	shl.b64 	%rd20, %rd19, 3;
	add.s64 	%rd21, %rd11, %rd20;
	ld.f64 	%fd7, [%rd21];
	add.f64 	%fd20, %fd20, %fd7;
	add.s64 	%rd5, %rd3, %rd4;
	setp.lt.s64	%p2, %rd5, %rd14;
	mov.u64 	%rd23, %rd5;
	@%p2 bra 	BB8_2;

BB8_3:
	add.s32 	%r67, %r3, 16;
	shr.s32 	%r68, %r67, 31;
	shr.u32 	%r69, %r68, 27;
	add.s32 	%r70, %r67, %r69;
	and.b32  	%r71, %r70, -32;
	sub.s32 	%r11, %r67, %r71;
	// inline asm
	mov.b64 {%r7,%r8}, %fd20;
	// inline asm
	mov.u32 	%r64, 31;
	// inline asm
	shfl.idx.b32 %r9, %r7, %r11, %r64;
	// inline asm
	// inline asm
	shfl.idx.b32 %r13, %r8, %r11, %r64;
	// inline asm
	// inline asm
	mov.b64 %fd9, {%r9,%r13};
	// inline asm
	add.f64 	%fd10, %fd20, %fd9;
	add.s32 	%r72, %r3, 8;
	shr.s32 	%r73, %r72, 31;
	shr.u32 	%r74, %r73, 27;
	add.s32 	%r75, %r72, %r74;
	and.b32  	%r76, %r75, -32;
	sub.s32 	%r23, %r72, %r76;
	// inline asm
	mov.b64 {%r19,%r20}, %fd10;
	// inline asm
	// inline asm
	shfl.idx.b32 %r21, %r19, %r23, %r64;
	// inline asm
	// inline asm
	shfl.idx.b32 %r25, %r20, %r23, %r64;
	// inline asm
	// inline asm
	mov.b64 %fd11, {%r21,%r25};
	// inline asm
	add.f64 	%fd12, %fd10, %fd11;
	add.s32 	%r77, %r3, 4;
	shr.s32 	%r78, %r77, 31;
	shr.u32 	%r79, %r78, 27;
	add.s32 	%r80, %r77, %r79;
	and.b32  	%r81, %r80, -32;
	sub.s32 	%r35, %r77, %r81;
	// inline asm
	mov.b64 {%r31,%r32}, %fd12;
	// inline asm
	// inline asm
	shfl.idx.b32 %r33, %r31, %r35, %r64;
	// inline asm
	// inline asm
	shfl.idx.b32 %r37, %r32, %r35, %r64;
	// inline asm
	// inline asm
	mov.b64 %fd13, {%r33,%r37};
	// inline asm
	add.f64 	%fd14, %fd12, %fd13;
	add.s32 	%r82, %r3, 2;
	shr.s32 	%r83, %r82, 31;
	shr.u32 	%r84, %r83, 27;
	add.s32 	%r85, %r82, %r84;
	and.b32  	%r86, %r85, -32;
	sub.s32 	%r47, %r82, %r86;
	// inline asm
	mov.b64 {%r43,%r44}, %fd14;
	// inline asm
	// inline asm
	shfl.idx.b32 %r45, %r43, %r47, %r64;
	// inline asm
	// inline asm
	shfl.idx.b32 %r49, %r44, %r47, %r64;
	// inline asm
	// inline asm
	mov.b64 %fd15, {%r45,%r49};
	// inline asm
	add.f64 	%fd16, %fd14, %fd15;
	cvt.u32.u64	%r87, %rd1;
	add.s32 	%r88, %r87, 1;
	shr.s32 	%r89, %r88, 31;
	shr.u32 	%r90, %r89, 27;
	add.s32 	%r91, %r88, %r90;
	and.b32  	%r92, %r91, -32;
	sub.s32 	%r59, %r88, %r92;
	// inline asm
	mov.b64 {%r55,%r56}, %fd16;
	// inline asm
	// inline asm
	shfl.idx.b32 %r57, %r55, %r59, %r64;
	// inline asm
	// inline asm
	shfl.idx.b32 %r61, %r56, %r59, %r64;
	// inline asm
	// inline asm
	mov.b64 %fd17, {%r57,%r61};
	// inline asm
	add.f64 	%fd4, %fd16, %fd17;
	and.b32  	%r93, %r2, 31;
	setp.ne.s32	%p3, %r93, 0;
	@%p3 bra 	BB8_6;

	ld.u64 	%rd24, [%rd12];

BB8_5:
	mov.u64 	%rd8, %rd24;
	mov.b64 	 %fd18, %rd8;
	add.f64 	%fd19, %fd4, %fd18;
	mov.b64 	 %rd22, %fd19;
	atom.cas.b64 	%rd24, [%rd12], %rd8, %rd22;
	setp.ne.s64	%p4, %rd8, %rd24;
	@%p4 bra 	BB8_5;

BB8_6:
	st.param.b64	[func_retval0+0], %rd12;
	ret;
}

	// .globl	_Z23deviceReduceArrayKernalPKlPKdPdll
.visible .func _Z23deviceReduceArrayKernalPKlPKdPdll(
	.param .b64 _Z23deviceReduceArrayKernalPKlPKdPdll_param_0,
	.param .b64 _Z23deviceReduceArrayKernalPKlPKdPdll_param_1,
	.param .b64 _Z23deviceReduceArrayKernalPKlPKdPdll_param_2,
	.param .b64 _Z23deviceReduceArrayKernalPKlPKdPdll_param_3,
	.param .b64 _Z23deviceReduceArrayKernalPKlPKdPdll_param_4
)
{
	.reg .pred 	%p<16>;
	.reg .s32 	%r<382>;
	.reg .f64 	%fd<109>;
	.reg .s64 	%rd<81>;


	ld.param.u64 	%rd34, [_Z23deviceReduceArrayKernalPKlPKdPdll_param_0];
	ld.param.u64 	%rd35, [_Z23deviceReduceArrayKernalPKlPKdPdll_param_1];
	ld.param.u64 	%rd36, [_Z23deviceReduceArrayKernalPKlPKdPdll_param_2];
	ld.param.u64 	%rd37, [_Z23deviceReduceArrayKernalPKlPKdPdll_param_3];
	ld.param.u64 	%rd38, [_Z23deviceReduceArrayKernalPKlPKdPdll_param_4];
	mov.u64 	%rd78, 0;
	setp.lt.s64	%p1, %rd37, 4;
	@%p1 bra 	BB9_14;

	mov.u32 	%r11, %ctaid.x;
	mov.u32 	%r12, %ntid.x;
	mov.u32 	%r13, %tid.x;
	mad.lo.s32 	%r14, %r12, %r11, %r13;
	add.s32 	%r15, %r14, 16;
	shr.s32 	%r16, %r15, 31;
	shr.u32 	%r17, %r16, 27;
	add.s32 	%r18, %r15, %r17;
	and.b32  	%r19, %r18, -32;
	sub.s32 	%r1, %r15, %r19;
	add.s32 	%r20, %r14, 8;
	shr.s32 	%r21, %r20, 31;
	shr.u32 	%r22, %r21, 27;
	add.s32 	%r23, %r20, %r22;
	and.b32  	%r24, %r23, -32;
	sub.s32 	%r2, %r20, %r24;
	add.s32 	%r25, %r14, 4;
	shr.s32 	%r26, %r25, 31;
	shr.u32 	%r27, %r26, 27;
	add.s32 	%r28, %r25, %r27;
	and.b32  	%r29, %r28, -32;
	sub.s32 	%r3, %r25, %r29;
	add.s32 	%r30, %r14, 2;
	shr.s32 	%r31, %r30, 31;
	shr.u32 	%r32, %r31, 27;
	add.s32 	%r33, %r30, %r32;
	and.b32  	%r34, %r33, -32;
	sub.s32 	%r4, %r30, %r34;
	add.s32 	%r35, %r14, 1;
	shr.s32 	%r36, %r35, 31;
	shr.u32 	%r37, %r36, 27;
	add.s32 	%r38, %r35, %r37;
	and.b32  	%r39, %r38, -32;
	sub.s32 	%r5, %r35, %r39;
	mov.u64 	%rd78, 0;

BB9_2:
	cvt.u64.u32	%rd73, %r14;
	mov.f64 	%fd106, 0d0000000000000000;
	mov.f64 	%fd103, %fd106;
	mov.f64 	%fd100, %fd106;
	mov.f64 	%fd97, %fd106;
	mov.f64 	%fd98, %fd106;
	mov.f64 	%fd101, %fd106;
	mov.f64 	%fd104, %fd106;
	mov.f64 	%fd107, %fd106;
	setp.ge.s64	%p2, %rd73, %rd38;
	@%p2 bra 	BB9_4;

BB9_3:
	shl.b64 	%rd41, %rd73, 3;
	add.s64 	%rd42, %rd34, %rd41;
	ld.u64 	%rd43, [%rd42];
	shr.u64 	%rd44, %rd43, 3;
	add.s64 	%rd45, %rd78, %rd44;
	shl.b64 	%rd46, %rd45, 3;
	add.s64 	%rd47, %rd46, %rd35;
	ld.f64 	%fd29, [%rd47+128];
	add.f64 	%fd98, %fd98, %fd29;
	ld.f64 	%fd30, [%rd47+136];
	add.f64 	%fd101, %fd101, %fd30;
	ld.f64 	%fd31, [%rd47+144];
	add.f64 	%fd104, %fd104, %fd31;
	ld.f64 	%fd32, [%rd47+152];
	add.f64 	%fd107, %fd107, %fd32;
	mov.u32 	%r45, %nctaid.x;
	mul.lo.s32 	%r46, %r45, %r12;
	cvt.u64.u32	%rd48, %r46;
	add.s64 	%rd73, %rd48, %rd73;
	setp.lt.s64	%p3, %rd73, %rd38;
	mov.f64 	%fd97, %fd98;
	mov.f64 	%fd100, %fd101;
	mov.f64 	%fd103, %fd104;
	mov.f64 	%fd106, %fd107;
	@%p3 bra 	BB9_3;

BB9_4:
	and.b32  	%r288, %r13, 31;
	// inline asm
	mov.b64 {%r47,%r48}, %fd97;
	// inline asm
	// inline asm
	mov.b64 {%r49,%r50}, %fd100;
	// inline asm
	// inline asm
	mov.b64 {%r51,%r52}, %fd103;
	// inline asm
	// inline asm
	mov.b64 {%r53,%r54}, %fd106;
	// inline asm
	mov.u32 	%r278, 31;
	// inline asm
	shfl.idx.b32 %r55, %r47, %r1, %r278;
	// inline asm
	// inline asm
	shfl.idx.b32 %r59, %r48, %r1, %r278;
	// inline asm
	// inline asm
	shfl.idx.b32 %r63, %r49, %r1, %r278;
	// inline asm
	// inline asm
	shfl.idx.b32 %r67, %r50, %r1, %r278;
	// inline asm
	// inline asm
	shfl.idx.b32 %r71, %r51, %r1, %r278;
	// inline asm
	// inline asm
	shfl.idx.b32 %r75, %r52, %r1, %r278;
	// inline asm
	// inline asm
	shfl.idx.b32 %r79, %r53, %r1, %r278;
	// inline asm
	// inline asm
	shfl.idx.b32 %r83, %r54, %r1, %r278;
	// inline asm
	// inline asm
	mov.b64 %fd37, {%r55,%r59};
	// inline asm
	// inline asm
	mov.b64 %fd38, {%r63,%r67};
	// inline asm
	// inline asm
	mov.b64 %fd39, {%r71,%r75};
	// inline asm
	// inline asm
	mov.b64 %fd40, {%r79,%r83};
	// inline asm
	add.f64 	%fd41, %fd97, %fd37;
	add.f64 	%fd42, %fd100, %fd38;
	add.f64 	%fd43, %fd103, %fd39;
	add.f64 	%fd44, %fd106, %fd40;
	// inline asm
	mov.b64 {%r95,%r96}, %fd41;
	// inline asm
	// inline asm
	mov.b64 {%r97,%r98}, %fd42;
	// inline asm
	// inline asm
	mov.b64 {%r99,%r100}, %fd43;
	// inline asm
	// inline asm
	mov.b64 {%r101,%r102}, %fd44;
	// inline asm
	// inline asm
	shfl.idx.b32 %r103, %r95, %r2, %r278;
	// inline asm
	// inline asm
	shfl.idx.b32 %r107, %r96, %r2, %r278;
	// inline asm
	// inline asm
	shfl.idx.b32 %r111, %r97, %r2, %r278;
	// inline asm
	// inline asm
	shfl.idx.b32 %r115, %r98, %r2, %r278;
	// inline asm
	// inline asm
	shfl.idx.b32 %r119, %r99, %r2, %r278;
	// inline asm
	// inline asm
	shfl.idx.b32 %r123, %r100, %r2, %r278;
	// inline asm
	// inline asm
	shfl.idx.b32 %r127, %r101, %r2, %r278;
	// inline asm
	// inline asm
	shfl.idx.b32 %r131, %r102, %r2, %r278;
	// inline asm
	// inline asm
	mov.b64 %fd45, {%r103,%r107};
	// inline asm
	// inline asm
	mov.b64 %fd46, {%r111,%r115};
	// inline asm
	// inline asm
	mov.b64 %fd47, {%r119,%r123};
	// inline asm
	// inline asm
	mov.b64 %fd48, {%r127,%r131};
	// inline asm
	add.f64 	%fd49, %fd41, %fd45;
	add.f64 	%fd50, %fd42, %fd46;
	add.f64 	%fd51, %fd43, %fd47;
	add.f64 	%fd52, %fd44, %fd48;
	// inline asm
	mov.b64 {%r143,%r144}, %fd49;
	// inline asm
	// inline asm
	mov.b64 {%r145,%r146}, %fd50;
	// inline asm
	// inline asm
	mov.b64 {%r147,%r148}, %fd51;
	// inline asm
	// inline asm
	mov.b64 {%r149,%r150}, %fd52;
	// inline asm
	// inline asm
	shfl.idx.b32 %r151, %r143, %r3, %r278;
	// inline asm
	// inline asm
	shfl.idx.b32 %r155, %r144, %r3, %r278;
	// inline asm
	// inline asm
	shfl.idx.b32 %r159, %r145, %r3, %r278;
	// inline asm
	// inline asm
	shfl.idx.b32 %r163, %r146, %r3, %r278;
	// inline asm
	// inline asm
	shfl.idx.b32 %r167, %r147, %r3, %r278;
	// inline asm
	// inline asm
	shfl.idx.b32 %r171, %r148, %r3, %r278;
	// inline asm
	// inline asm
	shfl.idx.b32 %r175, %r149, %r3, %r278;
	// inline asm
	// inline asm
	shfl.idx.b32 %r179, %r150, %r3, %r278;
	// inline asm
	// inline asm
	mov.b64 %fd53, {%r151,%r155};
	// inline asm
	// inline asm
	mov.b64 %fd54, {%r159,%r163};
	// inline asm
	// inline asm
	mov.b64 %fd55, {%r167,%r171};
	// inline asm
	// inline asm
	mov.b64 %fd56, {%r175,%r179};
	// inline asm
	add.f64 	%fd57, %fd49, %fd53;
	add.f64 	%fd58, %fd50, %fd54;
	add.f64 	%fd59, %fd51, %fd55;
	add.f64 	%fd60, %fd52, %fd56;
	// inline asm
	mov.b64 {%r191,%r192}, %fd57;
	// inline asm
	// inline asm
	mov.b64 {%r193,%r194}, %fd58;
	// inline asm
	// inline asm
	mov.b64 {%r195,%r196}, %fd59;
	// inline asm
	// inline asm
	mov.b64 {%r197,%r198}, %fd60;
	// inline asm
	// inline asm
	shfl.idx.b32 %r199, %r191, %r4, %r278;
	// inline asm
	// inline asm
	shfl.idx.b32 %r203, %r192, %r4, %r278;
	// inline asm
	// inline asm
	shfl.idx.b32 %r207, %r193, %r4, %r278;
	// inline asm
	// inline asm
	shfl.idx.b32 %r211, %r194, %r4, %r278;
	// inline asm
	// inline asm
	shfl.idx.b32 %r215, %r195, %r4, %r278;
	// inline asm
	// inline asm
	shfl.idx.b32 %r219, %r196, %r4, %r278;
	// inline asm
	// inline asm
	shfl.idx.b32 %r223, %r197, %r4, %r278;
	// inline asm
	// inline asm
	shfl.idx.b32 %r227, %r198, %r4, %r278;
	// inline asm
	// inline asm
	mov.b64 %fd61, {%r199,%r203};
	// inline asm
	// inline asm
	mov.b64 %fd62, {%r207,%r211};
	// inline asm
	// inline asm
	mov.b64 %fd63, {%r215,%r219};
	// inline asm
	// inline asm
	mov.b64 %fd64, {%r223,%r227};
	// inline asm
	add.f64 	%fd65, %fd57, %fd61;
	add.f64 	%fd66, %fd58, %fd62;
	add.f64 	%fd67, %fd59, %fd63;
	add.f64 	%fd68, %fd60, %fd64;
	// inline asm
	mov.b64 {%r239,%r240}, %fd65;
	// inline asm
	// inline asm
	mov.b64 {%r241,%r242}, %fd66;
	// inline asm
	// inline asm
	mov.b64 {%r243,%r244}, %fd67;
	// inline asm
	// inline asm
	mov.b64 {%r245,%r246}, %fd68;
	// inline asm
	// inline asm
	shfl.idx.b32 %r247, %r239, %r5, %r278;
	// inline asm
	// inline asm
	shfl.idx.b32 %r251, %r240, %r5, %r278;
	// inline asm
	// inline asm
	shfl.idx.b32 %r255, %r241, %r5, %r278;
	// inline asm
	// inline asm
	shfl.idx.b32 %r259, %r242, %r5, %r278;
	// inline asm
	// inline asm
	shfl.idx.b32 %r263, %r243, %r5, %r278;
	// inline asm
	// inline asm
	shfl.idx.b32 %r267, %r244, %r5, %r278;
	// inline asm
	// inline asm
	shfl.idx.b32 %r271, %r245, %r5, %r278;
	// inline asm
	// inline asm
	shfl.idx.b32 %r275, %r246, %r5, %r278;
	// inline asm
	// inline asm
	mov.b64 %fd69, {%r247,%r251};
	// inline asm
	// inline asm
	mov.b64 %fd70, {%r255,%r259};
	// inline asm
	// inline asm
	mov.b64 %fd71, {%r263,%r267};
	// inline asm
	// inline asm
	mov.b64 %fd72, {%r271,%r275};
	// inline asm
	add.f64 	%fd13, %fd65, %fd69;
	add.f64 	%fd14, %fd66, %fd70;
	add.f64 	%fd15, %fd67, %fd71;
	add.f64 	%fd16, %fd68, %fd72;
	setp.ne.s32	%p4, %r288, 0;
	@%p4 bra 	BB9_13;

	shl.b64 	%rd49, %rd78, 3;
	add.s64 	%rd50, %rd36, %rd49;
	ld.u64 	%rd74, [%rd50];

BB9_6:
	mov.u64 	%rd6, %rd74;
	mov.b64 	 %fd73, %rd6;
	add.f64 	%fd74, %fd13, %fd73;
	mov.b64 	 %rd51, %fd74;
	atom.cas.b64 	%rd74, [%rd50], %rd6, %rd51;
	setp.ne.s64	%p5, %rd6, %rd74;
	@%p5 bra 	BB9_6;

	add.s64 	%rd8, %rd50, 8;
	ld.u64 	%rd75, [%rd50+8];

BB9_8:
	mov.u64 	%rd10, %rd75;
	mov.b64 	 %fd75, %rd10;
	add.f64 	%fd76, %fd14, %fd75;
	mov.b64 	 %rd56, %fd76;
	atom.cas.b64 	%rd75, [%rd8], %rd10, %rd56;
	setp.ne.s64	%p6, %rd10, %rd75;
	@%p6 bra 	BB9_8;

	add.s64 	%rd12, %rd50, 16;
	ld.u64 	%rd76, [%rd50+16];

BB9_10:
	mov.u64 	%rd14, %rd76;
	mov.b64 	 %fd77, %rd14;
	add.f64 	%fd78, %fd15, %fd77;
	mov.b64 	 %rd59, %fd78;
	atom.cas.b64 	%rd76, [%rd12], %rd14, %rd59;
	setp.ne.s64	%p7, %rd14, %rd76;
	@%p7 bra 	BB9_10;

	add.s64 	%rd16, %rd50, 24;
	ld.u64 	%rd77, [%rd50+24];

BB9_12:
	mov.u64 	%rd18, %rd77;
	mov.b64 	 %fd79, %rd18;
	add.f64 	%fd80, %fd16, %fd79;
	mov.b64 	 %rd62, %fd80;
	atom.cas.b64 	%rd77, [%rd16], %rd18, %rd62;
	setp.ne.s64	%p8, %rd18, %rd77;
	@%p8 bra 	BB9_12;

BB9_13:
	add.s64 	%rd78, %rd78, 4;
	sub.s64 	%rd63, %rd37, %rd78;
	setp.gt.s64	%p9, %rd63, 3;
	@%p9 bra 	BB9_2;

BB9_14:
	setp.ge.s64	%p10, %rd78, %rd37;
	@%p10 bra 	BB9_23;

	mov.u32 	%r289, %ctaid.x;
	mov.u32 	%r290, %ntid.x;
	mov.u32 	%r291, %tid.x;
	mad.lo.s32 	%r292, %r290, %r289, %r291;
	cvt.u64.u32	%rd22, %r292;
	mov.u32 	%r293, %nctaid.x;
	mul.lo.s32 	%r294, %r293, %r290;
	cvt.u64.u32	%rd23, %r294;
	add.s32 	%r295, %r292, 16;
	shr.s32 	%r296, %r295, 31;
	shr.u32 	%r297, %r296, 27;
	add.s32 	%r298, %r295, %r297;
	and.b32  	%r299, %r298, -32;
	sub.s32 	%r6, %r295, %r299;
	add.s32 	%r300, %r292, 8;
	shr.s32 	%r301, %r300, 31;
	shr.u32 	%r302, %r301, 27;
	add.s32 	%r303, %r300, %r302;
	and.b32  	%r304, %r303, -32;
	sub.s32 	%r7, %r300, %r304;
	add.s32 	%r305, %r292, 4;
	shr.s32 	%r306, %r305, 31;
	shr.u32 	%r307, %r306, 27;
	add.s32 	%r308, %r305, %r307;
	and.b32  	%r309, %r308, -32;
	sub.s32 	%r8, %r305, %r309;
	add.s32 	%r310, %r292, 2;
	shr.s32 	%r311, %r310, 31;
	shr.u32 	%r312, %r311, 27;
	add.s32 	%r313, %r310, %r312;
	and.b32  	%r314, %r313, -32;
	sub.s32 	%r9, %r310, %r314;
	add.s32 	%r315, %r292, 1;
	shr.s32 	%r316, %r315, 31;
	shr.u32 	%r317, %r316, 27;
	add.s32 	%r318, %r315, %r317;
	and.b32  	%r319, %r318, -32;
	sub.s32 	%r10, %r315, %r319;

BB9_16:
	shl.b64 	%rd64, %rd78, 3;
	add.s64 	%rd25, %rd36, %rd64;
	mov.f64 	%fd108, 0d0000000000000000;
	setp.ge.s64	%p11, %rd22, %rd38;
	@%p11 bra 	BB9_19;

	add.s64 	%rd26, %rd78, 16;
	mov.f64 	%fd108, 0d0000000000000000;
	mov.u64 	%rd79, %rd22;

BB9_18:
	mov.u64 	%rd27, %rd79;
	shl.b64 	%rd65, %rd27, 3;
	add.s64 	%rd66, %rd34, %rd65;
	ld.u64 	%rd67, [%rd66];
	shr.u64 	%rd68, %rd67, 3;
	add.s64 	%rd69, %rd26, %rd68;
	shl.b64 	%rd70, %rd69, 3;
	add.s64 	%rd71, %rd35, %rd70;
	ld.f64 	%fd83, [%rd71];
	add.f64 	%fd108, %fd108, %fd83;
	add.s64 	%rd28, %rd23, %rd27;
	setp.lt.s64	%p12, %rd28, %rd38;
	mov.u64 	%rd79, %rd28;
	@%p12 bra 	BB9_18;

BB9_19:
	and.b32  	%r381, %r291, 31;
	// inline asm
	mov.b64 {%r320,%r321}, %fd108;
	// inline asm
	mov.u32 	%r377, 31;
	// inline asm
	shfl.idx.b32 %r322, %r320, %r6, %r377;
	// inline asm
	// inline asm
	shfl.idx.b32 %r326, %r321, %r6, %r377;
	// inline asm
	// inline asm
	mov.b64 %fd85, {%r322,%r326};
	// inline asm
	add.f64 	%fd86, %fd108, %fd85;
	// inline asm
	mov.b64 {%r332,%r333}, %fd86;
	// inline asm
	// inline asm
	shfl.idx.b32 %r334, %r332, %r7, %r377;
	// inline asm
	// inline asm
	shfl.idx.b32 %r338, %r333, %r7, %r377;
	// inline asm
	// inline asm
	mov.b64 %fd87, {%r334,%r338};
	// inline asm
	add.f64 	%fd88, %fd86, %fd87;
	// inline asm
	mov.b64 {%r344,%r345}, %fd88;
	// inline asm
	// inline asm
	shfl.idx.b32 %r346, %r344, %r8, %r377;
	// inline asm
	// inline asm
	shfl.idx.b32 %r350, %r345, %r8, %r377;
	// inline asm
	// inline asm
	mov.b64 %fd89, {%r346,%r350};
	// inline asm
	add.f64 	%fd90, %fd88, %fd89;
	// inline asm
	mov.b64 {%r356,%r357}, %fd90;
	// inline asm
	// inline asm
	shfl.idx.b32 %r358, %r356, %r9, %r377;
	// inline asm
	// inline asm
	shfl.idx.b32 %r362, %r357, %r9, %r377;
	// inline asm
	// inline asm
	mov.b64 %fd91, {%r358,%r362};
	// inline asm
	add.f64 	%fd92, %fd90, %fd91;
	// inline asm
	mov.b64 {%r368,%r369}, %fd92;
	// inline asm
	// inline asm
	shfl.idx.b32 %r370, %r368, %r10, %r377;
	// inline asm
	// inline asm
	shfl.idx.b32 %r374, %r369, %r10, %r377;
	// inline asm
	// inline asm
	mov.b64 %fd93, {%r370,%r374};
	// inline asm
	add.f64 	%fd20, %fd92, %fd93;
	setp.ne.s32	%p13, %r381, 0;
	@%p13 bra 	BB9_22;

	ld.u64 	%rd80, [%rd25];

BB9_21:
	mov.u64 	%rd31, %rd80;
	mov.b64 	 %fd94, %rd31;
	add.f64 	%fd95, %fd20, %fd94;
	mov.b64 	 %rd72, %fd95;
	atom.cas.b64 	%rd80, [%rd25], %rd31, %rd72;
	setp.ne.s64	%p14, %rd31, %rd80;
	@%p14 bra 	BB9_21;

BB9_22:
	add.s64 	%rd78, %rd78, 1;
	setp.lt.s64	%p15, %rd78, %rd37;
	@%p15 bra 	BB9_16;

BB9_23:
	ret;
}

	// .globl	_Z8identityPKiPil
.visible .entry _Z8identityPKiPil(
	.param .u64 _Z8identityPKiPil_param_0,
	.param .u64 _Z8identityPKiPil_param_1,
	.param .u64 _Z8identityPKiPil_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .s32 	%r<5>;
	.reg .s64 	%rd<12>;


	ld.param.u64 	%rd2, [_Z8identityPKiPil_param_0];
	ld.param.u64 	%rd3, [_Z8identityPKiPil_param_1];
	ld.param.u64 	%rd4, [_Z8identityPKiPil_param_2];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32	%rd5, %r1;
	mov.u32 	%r2, %ctaid.x;
	mov.u32 	%r3, %ntid.x;
	mul.wide.u32 	%rd6, %r3, %r2;
	add.s64 	%rd1, %rd6, %rd5;
	setp.ge.s64	%p1, %rd1, %rd4;
	@%p1 bra 	BB10_2;

	cvta.to.global.u64 	%rd7, %rd2;
	shl.b64 	%rd8, %rd1, 2;
	add.s64 	%rd9, %rd7, %rd8;
	ld.global.u32 	%r4, [%rd9];
	cvta.to.global.u64 	%rd10, %rd3;
	add.s64 	%rd11, %rd10, %rd8;
	st.global.u32 	[%rd11], %r4;

BB10_2:
	ret;
}

	// .globl	_Z16intArrayIdentityPKlPKiPlPil
.visible .entry _Z16intArrayIdentityPKlPKiPlPil(
	.param .u64 _Z16intArrayIdentityPKlPKiPlPil_param_0,
	.param .u64 _Z16intArrayIdentityPKlPKiPlPil_param_1,
	.param .u64 _Z16intArrayIdentityPKlPKiPlPil_param_2,
	.param .u64 _Z16intArrayIdentityPKlPKiPlPil_param_3,
	.param .u64 _Z16intArrayIdentityPKlPKiPlPil_param_4
)
{
	.reg .pred 	%p<4>;
	.reg .s32 	%r<5>;
	.reg .s64 	%rd<38>;


	ld.param.u64 	%rd15, [_Z16intArrayIdentityPKlPKiPlPil_param_0];
	ld.param.u64 	%rd16, [_Z16intArrayIdentityPKlPKiPlPil_param_1];
	ld.param.u64 	%rd17, [_Z16intArrayIdentityPKlPKiPlPil_param_2];
	ld.param.u64 	%rd18, [_Z16intArrayIdentityPKlPKiPlPil_param_3];
	ld.param.u64 	%rd19, [_Z16intArrayIdentityPKlPKiPlPil_param_4];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32	%rd20, %r1;
	mov.u32 	%r2, %ctaid.x;
	mov.u32 	%r3, %ntid.x;
	mul.wide.u32 	%rd21, %r3, %r2;
	add.s64 	%rd1, %rd21, %rd20;
	setp.ge.s64	%p1, %rd1, %rd19;
	@%p1 bra 	BB11_5;

	cvta.to.global.u64 	%rd2, %rd18;
	cvta.to.global.u64 	%rd3, %rd16;
	cvta.to.global.u64 	%rd22, %rd15;
	shl.b64 	%rd23, %rd1, 3;
	add.s64 	%rd24, %rd22, %rd23;
	ld.global.u64 	%rd4, [%rd24];
	shr.u64 	%rd5, %rd4, 2;
	shl.b64 	%rd25, %rd5, 2;
	add.s64 	%rd26, %rd3, %rd25;
	ld.global.u64 	%rd6, [%rd26];
	ld.global.u64 	%rd7, [%rd26+8];
	setp.lt.s64	%p2, %rd7, 1;
	@%p2 bra 	BB11_4;

	and.b64  	%rd28, %rd4, -4;
	add.s64 	%rd36, %rd28, 128;
	mov.u64 	%rd37, 0;

BB11_3:
	add.s64 	%rd29, %rd3, %rd36;
	ld.global.u32 	%r4, [%rd29];
	add.s64 	%rd30, %rd2, %rd36;
	st.global.u32 	[%rd30], %r4;
	add.s64 	%rd36, %rd36, 4;
	add.s64 	%rd37, %rd37, 1;
	setp.lt.s64	%p3, %rd37, %rd7;
	@%p3 bra 	BB11_3;

BB11_4:
	cvta.to.global.u64 	%rd31, %rd17;
	add.s64 	%rd33, %rd2, %rd25;
	add.s64 	%rd35, %rd31, %rd23;
	st.global.u64 	[%rd35], %rd4;
	st.global.u64 	[%rd33], %rd6;
	st.global.u64 	[%rd33+8], %rd7;

BB11_5:
	ret;
}

	// .globl	_Z20IntDataPointIdentityPKlPKiS2_PlPiS4_l
.visible .entry _Z20IntDataPointIdentityPKlPKiS2_PlPiS4_l(
	.param .u64 _Z20IntDataPointIdentityPKlPKiS2_PlPiS4_l_param_0,
	.param .u64 _Z20IntDataPointIdentityPKlPKiS2_PlPiS4_l_param_1,
	.param .u64 _Z20IntDataPointIdentityPKlPKiS2_PlPiS4_l_param_2,
	.param .u64 _Z20IntDataPointIdentityPKlPKiS2_PlPiS4_l_param_3,
	.param .u64 _Z20IntDataPointIdentityPKlPKiS2_PlPiS4_l_param_4,
	.param .u64 _Z20IntDataPointIdentityPKlPKiS2_PlPiS4_l_param_5,
	.param .u64 _Z20IntDataPointIdentityPKlPKiS2_PlPiS4_l_param_6
)
{
	.reg .pred 	%p<4>;
	.reg .s32 	%r<6>;
	.reg .s64 	%rd<45>;


	ld.param.u64 	%rd15, [_Z20IntDataPointIdentityPKlPKiS2_PlPiS4_l_param_0];
	ld.param.u64 	%rd16, [_Z20IntDataPointIdentityPKlPKiS2_PlPiS4_l_param_1];
	ld.param.u64 	%rd17, [_Z20IntDataPointIdentityPKlPKiS2_PlPiS4_l_param_2];
	ld.param.u64 	%rd18, [_Z20IntDataPointIdentityPKlPKiS2_PlPiS4_l_param_3];
	ld.param.u64 	%rd19, [_Z20IntDataPointIdentityPKlPKiS2_PlPiS4_l_param_4];
	ld.param.u64 	%rd20, [_Z20IntDataPointIdentityPKlPKiS2_PlPiS4_l_param_5];
	ld.param.u64 	%rd21, [_Z20IntDataPointIdentityPKlPKiS2_PlPiS4_l_param_6];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32	%rd22, %r1;
	mov.u32 	%r2, %ctaid.x;
	mov.u32 	%r3, %ntid.x;
	mul.wide.u32 	%rd23, %r3, %r2;
	add.s64 	%rd1, %rd23, %rd22;
	setp.ge.s64	%p1, %rd1, %rd21;
	@%p1 bra 	BB12_5;

	cvta.to.global.u64 	%rd2, %rd20;
	cvta.to.global.u64 	%rd3, %rd17;
	cvta.to.global.u64 	%rd24, %rd15;
	shl.b64 	%rd25, %rd1, 3;
	add.s64 	%rd26, %rd24, %rd25;
	ld.global.u64 	%rd4, [%rd26];
	shr.u64 	%rd5, %rd4, 2;
	shl.b64 	%rd27, %rd5, 2;
	add.s64 	%rd28, %rd3, %rd27;
	ld.global.u64 	%rd6, [%rd28];
	ld.global.u64 	%rd7, [%rd28+8];
	setp.lt.s64	%p2, %rd7, 1;
	@%p2 bra 	BB12_4;

	and.b64  	%rd30, %rd4, -4;
	add.s64 	%rd43, %rd30, 128;
	mov.u64 	%rd44, 0;

BB12_3:
	add.s64 	%rd31, %rd3, %rd43;
	ld.global.u32 	%r4, [%rd31];
	add.s64 	%rd32, %rd2, %rd43;
	st.global.u32 	[%rd32], %r4;
	add.s64 	%rd43, %rd43, 4;
	add.s64 	%rd44, %rd44, 1;
	setp.lt.s64	%p3, %rd44, %rd7;
	@%p3 bra 	BB12_3;

BB12_4:
	cvta.to.global.u64 	%rd33, %rd19;
	cvta.to.global.u64 	%rd34, %rd16;
	cvta.to.global.u64 	%rd35, %rd18;
	add.s64 	%rd37, %rd2, %rd27;
	add.s64 	%rd39, %rd35, %rd25;
	st.global.u64 	[%rd39], %rd4;
	st.global.u64 	[%rd37], %rd6;
	st.global.u64 	[%rd37+8], %rd7;
	shl.b64 	%rd40, %rd1, 2;
	add.s64 	%rd41, %rd34, %rd40;
	ld.global.u32 	%r5, [%rd41];
	add.s64 	%rd42, %rd33, %rd40;
	st.global.u32 	[%rd42], %r5;

BB12_5:
	ret;
}

	// .globl	_Z11intArrayAddPKlPKiPlPilS2_
.visible .entry _Z11intArrayAddPKlPKiPlPilS2_(
	.param .u64 _Z11intArrayAddPKlPKiPlPilS2__param_0,
	.param .u64 _Z11intArrayAddPKlPKiPlPilS2__param_1,
	.param .u64 _Z11intArrayAddPKlPKiPlPilS2__param_2,
	.param .u64 _Z11intArrayAddPKlPKiPlPilS2__param_3,
	.param .u64 _Z11intArrayAddPKlPKiPlPilS2__param_4,
	.param .u64 _Z11intArrayAddPKlPKiPlPilS2__param_5
)
{
	.reg .pred 	%p<4>;
	.reg .s32 	%r<7>;
	.reg .s64 	%rd<43>;


	ld.param.u64 	%rd18, [_Z11intArrayAddPKlPKiPlPilS2__param_0];
	ld.param.u64 	%rd19, [_Z11intArrayAddPKlPKiPlPilS2__param_1];
	ld.param.u64 	%rd20, [_Z11intArrayAddPKlPKiPlPilS2__param_2];
	ld.param.u64 	%rd21, [_Z11intArrayAddPKlPKiPlPilS2__param_3];
	ld.param.u64 	%rd23, [_Z11intArrayAddPKlPKiPlPilS2__param_4];
	ld.param.u64 	%rd22, [_Z11intArrayAddPKlPKiPlPilS2__param_5];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32	%rd24, %r1;
	mov.u32 	%r2, %ctaid.x;
	mov.u32 	%r3, %ntid.x;
	mul.wide.u32 	%rd25, %r3, %r2;
	add.s64 	%rd1, %rd25, %rd24;
	setp.ge.s64	%p1, %rd1, %rd23;
	@%p1 bra 	BB13_5;

	cvta.to.global.u64 	%rd2, %rd21;
	cvta.to.global.u64 	%rd3, %rd19;
	cvta.to.global.u64 	%rd26, %rd18;
	shl.b64 	%rd27, %rd1, 3;
	add.s64 	%rd28, %rd26, %rd27;
	ld.global.u64 	%rd4, [%rd28];
	shr.u64 	%rd5, %rd4, 2;
	shl.b64 	%rd29, %rd5, 2;
	add.s64 	%rd30, %rd3, %rd29;
	ld.global.u64 	%rd6, [%rd30];
	ld.global.u64 	%rd7, [%rd30+8];
	setp.lt.s64	%p2, %rd7, 1;
	@%p2 bra 	BB13_4;

	cvta.to.global.u64 	%rd40, %rd22;
	and.b64  	%rd32, %rd4, -4;
	add.s64 	%rd41, %rd32, 128;
	mov.u64 	%rd42, 0;

BB13_3:
	add.s64 	%rd33, %rd3, %rd41;
	ld.global.u32 	%r4, [%rd40];
	ld.global.u32 	%r5, [%rd33];
	add.s32 	%r6, %r4, %r5;
	add.s64 	%rd34, %rd2, %rd41;
	st.global.u32 	[%rd34], %r6;
	add.s64 	%rd41, %rd41, 4;
	add.s64 	%rd40, %rd40, 4;
	add.s64 	%rd42, %rd42, 1;
	setp.lt.s64	%p3, %rd42, %rd7;
	@%p3 bra 	BB13_3;

BB13_4:
	cvta.to.global.u64 	%rd35, %rd20;
	add.s64 	%rd37, %rd2, %rd29;
	add.s64 	%rd39, %rd35, %rd27;
	st.global.u64 	[%rd39], %rd4;
	st.global.u64 	[%rd37], %rd6;
	st.global.u64 	[%rd37+8], %rd7;

BB13_5:
	ret;
}

	// .globl	_Z12vectorLengthPKdS0_Pdl
.visible .entry _Z12vectorLengthPKdS0_Pdl(
	.param .u64 _Z12vectorLengthPKdS0_Pdl_param_0,
	.param .u64 _Z12vectorLengthPKdS0_Pdl_param_1,
	.param .u64 _Z12vectorLengthPKdS0_Pdl_param_2,
	.param .u64 _Z12vectorLengthPKdS0_Pdl_param_3
)
{
	.reg .pred 	%p<2>;
	.reg .s32 	%r<4>;
	.reg .f64 	%fd<6>;
	.reg .s64 	%rd<15>;


	ld.param.u64 	%rd2, [_Z12vectorLengthPKdS0_Pdl_param_0];
	ld.param.u64 	%rd3, [_Z12vectorLengthPKdS0_Pdl_param_1];
	ld.param.u64 	%rd4, [_Z12vectorLengthPKdS0_Pdl_param_2];
	ld.param.u64 	%rd5, [_Z12vectorLengthPKdS0_Pdl_param_3];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32	%rd6, %r1;
	mov.u32 	%r2, %ctaid.x;
	mov.u32 	%r3, %ntid.x;
	mul.wide.u32 	%rd7, %r3, %r2;
	add.s64 	%rd1, %rd7, %rd6;
	setp.ge.s64	%p1, %rd1, %rd5;
	@%p1 bra 	BB14_2;

	cvta.to.global.u64 	%rd8, %rd2;
	shl.b64 	%rd9, %rd1, 3;
	add.s64 	%rd10, %rd8, %rd9;
	ld.global.f64 	%fd1, [%rd10];
	cvta.to.global.u64 	%rd11, %rd3;
	add.s64 	%rd12, %rd11, %rd9;
	ld.global.f64 	%fd2, [%rd12];
	mul.f64 	%fd3, %fd2, %fd2;
	fma.rn.f64 	%fd4, %fd1, %fd1, %fd3;
	sqrt.rn.f64 	%fd5, %fd4;
	cvta.to.global.u64 	%rd13, %rd4;
	add.s64 	%rd14, %rd13, %rd9;
	st.global.f64 	[%rd14], %fd5;

BB14_2:
	ret;
}

	// .globl	_Z9plusMinusPKdPKfPdPfl
.visible .entry _Z9plusMinusPKdPKfPdPfl(
	.param .u64 _Z9plusMinusPKdPKfPdPfl_param_0,
	.param .u64 _Z9plusMinusPKdPKfPdPfl_param_1,
	.param .u64 _Z9plusMinusPKdPKfPdPfl_param_2,
	.param .u64 _Z9plusMinusPKdPKfPdPfl_param_3,
	.param .u64 _Z9plusMinusPKdPKfPdPfl_param_4
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<4>;
	.reg .s32 	%r<4>;
	.reg .f64 	%fd<7>;
	.reg .s64 	%rd<19>;


	ld.param.u64 	%rd2, [_Z9plusMinusPKdPKfPdPfl_param_0];
	ld.param.u64 	%rd3, [_Z9plusMinusPKdPKfPdPfl_param_1];
	ld.param.u64 	%rd4, [_Z9plusMinusPKdPKfPdPfl_param_2];
	ld.param.u64 	%rd5, [_Z9plusMinusPKdPKfPdPfl_param_3];
	ld.param.u64 	%rd6, [_Z9plusMinusPKdPKfPdPfl_param_4];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32	%rd7, %r1;
	mov.u32 	%r2, %ctaid.x;
	mov.u32 	%r3, %ntid.x;
	mul.wide.u32 	%rd8, %r3, %r2;
	add.s64 	%rd1, %rd8, %rd7;
	setp.ge.s64	%p1, %rd1, %rd6;
	@%p1 bra 	BB15_2;

	cvta.to.global.u64 	%rd9, %rd2;
	shl.b64 	%rd10, %rd1, 3;
	add.s64 	%rd11, %rd9, %rd10;
	cvta.to.global.u64 	%rd12, %rd3;
	shl.b64 	%rd13, %rd1, 2;
	add.s64 	%rd14, %rd12, %rd13;
	ld.global.f32 	%f1, [%rd14];
	cvt.ftz.f64.f32	%fd1, %f1;
	ld.global.f64 	%fd2, [%rd11];
	sub.f64 	%fd3, %fd2, %fd1;
	cvta.to.global.u64 	%rd15, %rd4;
	add.s64 	%rd16, %rd15, %rd10;
	st.global.f64 	[%rd16], %fd3;
	ld.global.f32 	%f2, [%rd14];
	cvt.ftz.f64.f32	%fd4, %f2;
	ld.global.f64 	%fd5, [%rd11];
	add.f64 	%fd6, %fd5, %fd4;
	cvt.rn.ftz.f32.f64	%f3, %fd6;
	cvta.to.global.u64 	%rd17, %rd5;
	add.s64 	%rd18, %rd17, %rd13;
	st.global.f32 	[%rd18], %f3;

BB15_2:
	ret;
}

	// .globl	_Z19applyLinearFunctionPKsPslss
.visible .entry _Z19applyLinearFunctionPKsPslss(
	.param .u64 _Z19applyLinearFunctionPKsPslss_param_0,
	.param .u64 _Z19applyLinearFunctionPKsPslss_param_1,
	.param .u64 _Z19applyLinearFunctionPKsPslss_param_2,
	.param .u16 _Z19applyLinearFunctionPKsPslss_param_3,
	.param .u16 _Z19applyLinearFunctionPKsPslss_param_4
)
{
	.reg .pred 	%p<2>;
	.reg .s16 	%rs<3>;
	.reg .s32 	%r<8>;
	.reg .s64 	%rd<12>;


	ld.param.u64 	%rd2, [_Z19applyLinearFunctionPKsPslss_param_0];
	ld.param.u64 	%rd3, [_Z19applyLinearFunctionPKsPslss_param_1];
	ld.param.u64 	%rd4, [_Z19applyLinearFunctionPKsPslss_param_2];
	ld.param.u16 	%rs1, [_Z19applyLinearFunctionPKsPslss_param_3];
	ld.param.u16 	%rs2, [_Z19applyLinearFunctionPKsPslss_param_4];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32	%rd5, %r1;
	mov.u32 	%r2, %ctaid.x;
	mov.u32 	%r3, %ntid.x;
	mul.wide.u32 	%rd6, %r3, %r2;
	add.s64 	%rd1, %rd6, %rd5;
	setp.ge.s64	%p1, %rd1, %rd4;
	@%p1 bra 	BB16_2;

	cvta.to.global.u64 	%rd7, %rd2;
	shl.b64 	%rd8, %rd1, 1;
	add.s64 	%rd9, %rd7, %rd8;
	ld.global.s16 	%r4, [%rd9];
	cvt.s32.s16	%r5, %rs2;
	cvt.u32.u16	%r6, %rs1;
	mad.lo.s32 	%r7, %r4, %r5, %r6;
	cvta.to.global.u64 	%rd10, %rd3;
	add.s64 	%rd11, %rd10, %rd8;
	st.global.u16 	[%rd11], %r7;

BB16_2:
	ret;
}

	// .globl	_Z8blockXORPKcPcll
.visible .entry _Z8blockXORPKcPcll(
	.param .u64 _Z8blockXORPKcPcll_param_0,
	.param .u64 _Z8blockXORPKcPcll_param_1,
	.param .u64 _Z8blockXORPKcPcll_param_2,
	.param .u64 _Z8blockXORPKcPcll_param_3
)
{
	.reg .pred 	%p<2>;
	.reg .s32 	%r<4>;
	.reg .s64 	%rd<16>;


	ld.param.u64 	%rd2, [_Z8blockXORPKcPcll_param_0];
	ld.param.u64 	%rd3, [_Z8blockXORPKcPcll_param_1];
	ld.param.u64 	%rd5, [_Z8blockXORPKcPcll_param_2];
	ld.param.u64 	%rd4, [_Z8blockXORPKcPcll_param_3];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32	%rd6, %r1;
	mov.u32 	%r2, %ctaid.x;
	mov.u32 	%r3, %ntid.x;
	mul.wide.u32 	%rd7, %r3, %r2;
	add.s64 	%rd1, %rd7, %rd6;
	shl.b64 	%rd8, %rd1, 3;
	setp.ge.s64	%p1, %rd8, %rd5;
	@%p1 bra 	BB17_2;

	cvta.to.global.u64 	%rd9, %rd2;
	add.s64 	%rd11, %rd9, %rd8;
	ld.global.u64 	%rd12, [%rd11];
	xor.b64  	%rd13, %rd12, %rd4;
	cvta.to.global.u64 	%rd14, %rd3;
	add.s64 	%rd15, %rd14, %rd8;
	st.global.u64 	[%rd15], %rd13;

BB17_2:
	ret;
}

	// .globl	_Z11multiplyBy2PiS_l
.visible .entry _Z11multiplyBy2PiS_l(
	.param .u64 _Z11multiplyBy2PiS_l_param_0,
	.param .u64 _Z11multiplyBy2PiS_l_param_1,
	.param .u64 _Z11multiplyBy2PiS_l_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .s32 	%r<7>;
	.reg .s64 	%rd<10>;


	ld.param.u64 	%rd1, [_Z11multiplyBy2PiS_l_param_0];
	ld.param.u64 	%rd2, [_Z11multiplyBy2PiS_l_param_1];
	ld.param.u64 	%rd3, [_Z11multiplyBy2PiS_l_param_2];
	mov.u32 	%r2, %tid.x;
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r4, %ctaid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r2;
	cvt.s64.s32	%rd4, %r1;
	setp.ge.s64	%p1, %rd4, %rd3;
	@%p1 bra 	BB18_2;

	cvta.to.global.u64 	%rd5, %rd1;
	mul.wide.s32 	%rd6, %r1, 4;
	add.s64 	%rd7, %rd5, %rd6;
	ld.global.u32 	%r5, [%rd7];
	shl.b32 	%r6, %r5, 1;
	cvta.to.global.u64 	%rd8, %rd2;
	add.s64 	%rd9, %rd8, %rd6;
	st.global.u32 	[%rd9], %r6;

BB18_2:
	ret;
}

	// .globl	_Z3sumPiS_lii
.visible .entry _Z3sumPiS_lii(
	.param .u64 _Z3sumPiS_lii_param_0,
	.param .u64 _Z3sumPiS_lii_param_1,
	.param .u64 _Z3sumPiS_lii_param_2,
	.param .u32 _Z3sumPiS_lii_param_3,
	.param .u32 _Z3sumPiS_lii_param_4
)
{
	.reg .pred 	%p<8>;
	.reg .s32 	%r<22>;
	.reg .s64 	%rd<38>;


	ld.param.u64 	%rd15, [_Z3sumPiS_lii_param_0];
	ld.param.u64 	%rd13, [_Z3sumPiS_lii_param_1];
	ld.param.u64 	%rd14, [_Z3sumPiS_lii_param_2];
	ld.param.u32 	%r9, [_Z3sumPiS_lii_param_3];
	cvta.to.global.u64 	%rd34, %rd15;
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32	%rd16, %r1;
	mov.u32 	%r2, %ctaid.x;
	mov.u32 	%r3, %ntid.x;
	mul.wide.u32 	%rd17, %r3, %r2;
	add.s64 	%rd2, %rd17, %rd16;
	setp.eq.s32	%p1, %r9, 0;
	@%p1 bra 	BB19_5;

	setp.ne.s64	%p2, %rd2, 0;
	@%p2 bra 	BB19_11;

	mov.u64 	%rd19, 16384;
	min.s64 	%rd3, %rd14, %rd19;
	mov.u32 	%r19, 0;
	mov.u32 	%r20, %r19;
	mov.u64 	%rd35, 0;
	setp.lt.s64	%p3, %rd3, 1;
	@%p3 bra 	BB19_4;

BB19_3:
	ld.global.u32 	%r12, [%rd34];
	add.s32 	%r20, %r12, %r20;
	add.s64 	%rd34, %rd34, 4;
	add.s64 	%rd35, %rd35, 1;
	setp.lt.s64	%p4, %rd35, %rd3;
	mov.u32 	%r19, %r20;
	@%p4 bra 	BB19_3;

BB19_4:
	cvta.to.global.u64 	%rd20, %rd13;
	st.global.u32 	[%rd20], %r19;
	bra.uni 	BB19_11;

BB19_5:
	setp.ge.s64	%p5, %rd2, %rd14;
	@%p5 bra 	BB19_11;

	mov.u32 	%r13, %nctaid.x;
	mul.lo.s32 	%r14, %r13, %r3;
	setp.eq.s32	%p6, %r14, 16384;
	@%p6 bra 	BB19_8;

	mov.u64 	%rd21, $str;
	cvta.global.u64 	%rd22, %rd21;
	mov.u64 	%rd23, $str1;
	cvta.global.u64 	%rd24, %rd23;
	mov.u64 	%rd25, __T20;
	cvta.global.u64 	%rd26, %rd25;
	mov.u32 	%r15, 138;
	mov.u64 	%rd27, 1;
	// Callseq Start 0
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.b64	[param0+0], %rd22;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd24;
	.param .b32 param2;
	st.param.b32	[param2+0], %r15;
	.param .b64 param3;
	st.param.b64	[param3+0], %rd26;
	.param .b64 param4;
	st.param.b64	[param4+0], %rd27;
	call.uni 
	__assertfail, 
	(
	param0, 
	param1, 
	param2, 
	param3, 
	param4
	);
	
	//{
	}// Callseq End 0

BB19_8:
	add.s64 	%rd30, %rd17, %rd16;
	shl.b64 	%rd31, %rd30, 2;
	add.s64 	%rd36, %rd34, %rd31;
	mov.u32 	%r21, 0;
	mov.u64 	%rd37, %rd2;

BB19_9:
	mov.u64 	%rd10, %rd37;
	ld.global.u32 	%r17, [%rd36];
	add.s32 	%r21, %r17, %r21;
	add.s64 	%rd36, %rd36, 65536;
	add.s64 	%rd12, %rd10, 16384;
	setp.lt.s64	%p7, %rd12, %rd14;
	mov.u64 	%rd37, %rd12;
	@%p7 bra 	BB19_9;

	shl.b64 	%rd32, %rd2, 2;
	add.s64 	%rd33, %rd34, %rd32;
	st.global.u32 	[%rd33], %r21;

BB19_11:
	ret;
}

	// .globl	_Z11intArraySumPKlPKiPlPilii
.visible .entry _Z11intArraySumPKlPKiPlPilii(
	.param .u64 _Z11intArraySumPKlPKiPlPilii_param_0,
	.param .u64 _Z11intArraySumPKlPKiPlPilii_param_1,
	.param .u64 _Z11intArraySumPKlPKiPlPilii_param_2,
	.param .u64 _Z11intArraySumPKlPKiPlPilii_param_3,
	.param .u64 _Z11intArraySumPKlPKiPlPilii_param_4,
	.param .u32 _Z11intArraySumPKlPKiPlPilii_param_5,
	.param .u32 _Z11intArraySumPKlPKiPlPilii_param_6
)
{
	.reg .pred 	%p<17>;
	.reg .s32 	%r<15>;
	.reg .s64 	%rd<100>;


	ld.param.u64 	%rd43, [_Z11intArraySumPKlPKiPlPilii_param_0];
	ld.param.u64 	%rd46, [_Z11intArraySumPKlPKiPlPilii_param_1];
	ld.param.u64 	%rd44, [_Z11intArraySumPKlPKiPlPilii_param_2];
	ld.param.u64 	%rd47, [_Z11intArraySumPKlPKiPlPilii_param_3];
	ld.param.u64 	%rd45, [_Z11intArraySumPKlPKiPlPilii_param_4];
	ld.param.u32 	%r2, [_Z11intArraySumPKlPKiPlPilii_param_5];
	cvta.to.global.u64 	%rd1, %rd47;
	cvta.to.global.u64 	%rd2, %rd46;
	cvta.to.global.u64 	%rd3, %rd43;
	mov.u32 	%r3, %tid.x;
	cvt.u64.u32	%rd48, %r3;
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r1, %ntid.x;
	mul.wide.u32 	%rd49, %r1, %r4;
	add.s64 	%rd4, %rd49, %rd48;
	setp.eq.s32	%p1, %r2, 0;
	@%p1 bra 	BB20_12;

	setp.ne.s64	%p2, %rd4, 0;
	@%p2 bra 	BB20_21;

	mov.u64 	%rd52, 16384;
	min.s64 	%rd5, %rd45, %rd52;
	ld.global.u64 	%rd6, [%rd3];
	mov.u64 	%rd51, 0;
	mov.u64 	%rd94, %rd51;
	setp.lt.s64	%p3, %rd5, 1;
	mov.u64 	%rd95, %rd51;
	@%p3 bra 	BB20_10;

	and.b64  	%rd54, %rd6, -4;
	add.s64 	%rd55, %rd54, %rd1;
	add.s64 	%rd7, %rd55, 128;
	add.s64 	%rd8, %rd2, 128;
	mov.u64 	%rd53, 0;
	mov.u64 	%rd9, %rd7;
	mov.u64 	%rd86, %rd6;
	mov.u64 	%rd90, %rd53;
	bra.uni 	BB20_4;

BB20_11:
	shl.b64 	%rd66, %rd90, 3;
	add.s64 	%rd67, %rd3, %rd66;
	ld.global.u64 	%rd28, [%rd67];
	mov.u64 	%rd86, %rd28;

BB20_4:
	mov.u64 	%rd10, %rd86;
	and.b64  	%rd57, %rd10, -4;
	add.s64 	%rd58, %rd2, %rd57;
	ld.global.u64 	%rd12, [%rd58];
	ld.global.u64 	%rd94, [%rd58+8];
	setp.gt.s64	%p4, %rd94, 0;
	setp.eq.s64	%p5, %rd90, 0;
	and.pred  	%p6, %p5, %p4;
	mov.u64 	%rd87, %rd7;
	mov.u64 	%rd89, %rd53;
	@!%p6 bra 	BB20_6;
	bra.uni 	BB20_5;

BB20_5:
	mov.u64 	%rd15, %rd89;
	mov.u64 	%rd14, %rd87;
	mov.u32 	%r5, 0;
	st.global.u32 	[%rd14], %r5;
	add.s64 	%rd16, %rd14, 4;
	add.s64 	%rd17, %rd15, 1;
	setp.lt.s64	%p7, %rd17, %rd94;
	mov.u64 	%rd87, %rd16;
	mov.u64 	%rd89, %rd17;
	@%p7 bra 	BB20_5;

BB20_6:
	setp.lt.s64	%p8, %rd94, 1;
	@%p8 bra 	BB20_9;

	add.s64 	%rd92, %rd8, %rd57;
	mov.u64 	%rd93, 0;
	mov.u64 	%rd91, %rd9;

BB20_8:
	mov.u64 	%rd19, %rd91;
	ld.global.u32 	%r6, [%rd19];
	ld.global.u32 	%r7, [%rd92];
	add.s32 	%r8, %r6, %r7;
	st.global.u32 	[%rd19], %r8;
	add.s64 	%rd92, %rd92, 4;
	add.s64 	%rd23, %rd19, 4;
	add.s64 	%rd93, %rd93, 1;
	setp.lt.s64	%p9, %rd93, %rd94;
	mov.u64 	%rd91, %rd23;
	@%p9 bra 	BB20_8;

BB20_9:
	add.s64 	%rd90, %rd90, 1;
	setp.lt.s64	%p10, %rd90, %rd5;
	mov.u64 	%rd95, %rd12;
	@%p10 bra 	BB20_11;

BB20_10:
	mov.u64 	%rd27, %rd95;
	and.b64  	%rd61, %rd6, -4;
	add.s64 	%rd62, %rd1, %rd61;
	cvta.to.global.u64 	%rd63, %rd44;
	st.global.u64 	[%rd63], %rd51;
	st.global.u64 	[%rd62], %rd27;
	st.global.u64 	[%rd62+8], %rd94;
	bra.uni 	BB20_21;

BB20_12:
	setp.ge.s64	%p11, %rd4, %rd45;
	@%p11 bra 	BB20_21;

	mov.u32 	%r9, %nctaid.x;
	mul.lo.s32 	%r10, %r9, %r1;
	setp.eq.s32	%p12, %r10, 16384;
	@%p12 bra 	BB20_15;

	mov.u64 	%rd68, $str;
	cvta.global.u64 	%rd69, %rd68;
	mov.u64 	%rd70, $str1;
	cvta.global.u64 	%rd71, %rd70;
	mov.u64 	%rd72, __T21;
	cvta.global.u64 	%rd73, %rd72;
	mov.u32 	%r11, 161;
	mov.u64 	%rd74, 1;
	// Callseq Start 1
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.b64	[param0+0], %rd69;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd71;
	.param .b32 param2;
	st.param.b32	[param2+0], %r11;
	.param .b64 param3;
	st.param.b64	[param3+0], %rd73;
	.param .b64 param4;
	st.param.b64	[param4+0], %rd74;
	call.uni 
	__assertfail, 
	(
	param0, 
	param1, 
	param2, 
	param3, 
	param4
	);
	
	//{
	}// Callseq End 1

BB20_15:
	add.s64 	%rd96, %rd4, 16384;
	setp.ge.s64	%p13, %rd96, %rd45;
	@%p13 bra 	BB20_21;

	shl.b64 	%rd75, %rd4, 3;
	add.s64 	%rd76, %rd3, %rd75;
	ld.global.u64 	%rd77, [%rd76];
	and.b64  	%rd78, %rd77, -4;
	add.s64 	%rd79, %rd78, %rd2;
	add.s64 	%rd30, %rd2, 128;
	add.s64 	%rd31, %rd79, 128;

BB20_17:
	shl.b64 	%rd80, %rd96, 3;
	add.s64 	%rd81, %rd3, %rd80;
	ld.global.u64 	%rd33, [%rd81];
	and.b64  	%rd82, %rd33, -4;
	add.s64 	%rd83, %rd82, %rd2;
	ld.global.u64 	%rd34, [%rd83+8];
	setp.lt.s64	%p14, %rd34, 1;
	@%p14 bra 	BB20_20;

	add.s64 	%rd98, %rd30, %rd82;
	mov.u64 	%rd99, 0;
	mov.u64 	%rd97, %rd31;

BB20_19:
	mov.u64 	%rd36, %rd97;
	ld.global.u32 	%r12, [%rd36];
	ld.global.u32 	%r13, [%rd98];
	add.s32 	%r14, %r12, %r13;
	st.global.u32 	[%rd36], %r14;
	add.s64 	%rd98, %rd98, 4;
	add.s64 	%rd40, %rd36, 4;
	add.s64 	%rd99, %rd99, 1;
	setp.lt.s64	%p15, %rd99, %rd34;
	mov.u64 	%rd97, %rd40;
	@%p15 bra 	BB20_19;

BB20_20:
	add.s64 	%rd96, %rd96, 16384;
	setp.lt.s64	%p16, %rd96, %rd45;
	@%p16 bra 	BB20_17;

BB20_21:
	ret;
}

	// .globl	_Z12DataPointMapPKlPKiPKdPlPdlS4_
.visible .entry _Z12DataPointMapPKlPKiPKdPlPdlS4_(
	.param .u64 _Z12DataPointMapPKlPKiPKdPlPdlS4__param_0,
	.param .u64 _Z12DataPointMapPKlPKiPKdPlPdlS4__param_1,
	.param .u64 _Z12DataPointMapPKlPKiPKdPlPdlS4__param_2,
	.param .u64 _Z12DataPointMapPKlPKiPKdPlPdlS4__param_3,
	.param .u64 _Z12DataPointMapPKlPKiPKdPlPdlS4__param_4,
	.param .u64 _Z12DataPointMapPKlPKiPKdPlPdlS4__param_5,
	.param .u64 _Z12DataPointMapPKlPKiPKdPlPdlS4__param_6
)
{
	.reg .pred 	%p<4>;
	.reg .s32 	%r<4>;
	.reg .f64 	%fd<4>;
	.reg .s64 	%rd<43>;


	ld.param.u64 	%rd18, [_Z12DataPointMapPKlPKiPKdPlPdlS4__param_0];
	ld.param.u64 	%rd19, [_Z12DataPointMapPKlPKiPKdPlPdlS4__param_2];
	ld.param.u64 	%rd20, [_Z12DataPointMapPKlPKiPKdPlPdlS4__param_3];
	ld.param.u64 	%rd21, [_Z12DataPointMapPKlPKiPKdPlPdlS4__param_4];
	ld.param.u64 	%rd23, [_Z12DataPointMapPKlPKiPKdPlPdlS4__param_5];
	ld.param.u64 	%rd22, [_Z12DataPointMapPKlPKiPKdPlPdlS4__param_6];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32	%rd24, %r1;
	mov.u32 	%r2, %ctaid.x;
	mov.u32 	%r3, %ntid.x;
	mul.wide.u32 	%rd25, %r3, %r2;
	add.s64 	%rd1, %rd25, %rd24;
	setp.ge.s64	%p1, %rd1, %rd23;
	@%p1 bra 	BB21_5;

	cvta.to.global.u64 	%rd2, %rd21;
	cvta.to.global.u64 	%rd3, %rd19;
	cvta.to.global.u64 	%rd26, %rd18;
	shl.b64 	%rd27, %rd1, 3;
	add.s64 	%rd28, %rd26, %rd27;
	ld.global.u64 	%rd4, [%rd28];
	shr.u64 	%rd5, %rd4, 3;
	shl.b64 	%rd29, %rd5, 3;
	add.s64 	%rd30, %rd3, %rd29;
	ld.global.u64 	%rd6, [%rd30];
	ld.global.u64 	%rd7, [%rd30+8];
	setp.lt.s64	%p2, %rd7, 1;
	@%p2 bra 	BB21_4;

	cvta.to.global.u64 	%rd40, %rd22;
	and.b64  	%rd32, %rd4, -8;
	add.s64 	%rd41, %rd32, 128;
	mov.u64 	%rd42, 0;

BB21_3:
	add.s64 	%rd33, %rd3, %rd41;
	ld.global.f64 	%fd1, [%rd40];
	ld.global.f64 	%fd2, [%rd33];
	add.f64 	%fd3, %fd2, %fd1;
	add.s64 	%rd34, %rd2, %rd41;
	st.global.f64 	[%rd34], %fd3;
	add.s64 	%rd41, %rd41, 8;
	add.s64 	%rd40, %rd40, 8;
	add.s64 	%rd42, %rd42, 1;
	setp.lt.s64	%p3, %rd42, %rd7;
	@%p3 bra 	BB21_3;

BB21_4:
	cvta.to.global.u64 	%rd35, %rd20;
	add.s64 	%rd37, %rd2, %rd29;
	add.s64 	%rd39, %rd35, %rd27;
	st.global.u64 	[%rd39], %rd4;
	st.global.u64 	[%rd37], %rd6;
	st.global.u64 	[%rd37+8], %rd7;

BB21_5:
	ret;
}

	// .globl	_Z15DataPointReducePKlPKdPlPdlii
.visible .entry _Z15DataPointReducePKlPKdPlPdlii(
	.param .u64 _Z15DataPointReducePKlPKdPlPdlii_param_0,
	.param .u64 _Z15DataPointReducePKlPKdPlPdlii_param_1,
	.param .u64 _Z15DataPointReducePKlPKdPlPdlii_param_2,
	.param .u64 _Z15DataPointReducePKlPKdPlPdlii_param_3,
	.param .u64 _Z15DataPointReducePKlPKdPlPdlii_param_4,
	.param .u32 _Z15DataPointReducePKlPKdPlPdlii_param_5,
	.param .u32 _Z15DataPointReducePKlPKdPlPdlii_param_6
)
{
	.reg .pred 	%p<17>;
	.reg .s32 	%r<8>;
	.reg .f64 	%fd<7>;
	.reg .s64 	%rd<101>;


	ld.param.u64 	%rd43, [_Z15DataPointReducePKlPKdPlPdlii_param_0];
	ld.param.u64 	%rd46, [_Z15DataPointReducePKlPKdPlPdlii_param_1];
	ld.param.u64 	%rd44, [_Z15DataPointReducePKlPKdPlPdlii_param_2];
	ld.param.u64 	%rd47, [_Z15DataPointReducePKlPKdPlPdlii_param_3];
	ld.param.u64 	%rd45, [_Z15DataPointReducePKlPKdPlPdlii_param_4];
	ld.param.u32 	%r2, [_Z15DataPointReducePKlPKdPlPdlii_param_5];
	cvta.to.global.u64 	%rd1, %rd47;
	cvta.to.global.u64 	%rd2, %rd46;
	cvta.to.global.u64 	%rd3, %rd43;
	mov.u32 	%r3, %tid.x;
	cvt.u64.u32	%rd48, %r3;
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r1, %ntid.x;
	mul.wide.u32 	%rd49, %r1, %r4;
	add.s64 	%rd4, %rd49, %rd48;
	setp.eq.s32	%p1, %r2, 0;
	@%p1 bra 	BB22_12;

	setp.ne.s64	%p2, %rd4, 0;
	@%p2 bra 	BB22_21;

	mov.u64 	%rd52, 16384;
	min.s64 	%rd5, %rd45, %rd52;
	ld.global.u64 	%rd6, [%rd3];
	mov.u64 	%rd51, 0;
	mov.u64 	%rd95, %rd51;
	setp.lt.s64	%p3, %rd5, 1;
	mov.u64 	%rd96, %rd51;
	@%p3 bra 	BB22_10;

	and.b64  	%rd54, %rd6, -8;
	add.s64 	%rd55, %rd54, %rd1;
	add.s64 	%rd7, %rd55, 128;
	add.s64 	%rd8, %rd2, 128;
	mov.u64 	%rd53, 0;
	mov.u64 	%rd9, %rd7;
	mov.u64 	%rd87, %rd6;
	mov.u64 	%rd91, %rd53;
	bra.uni 	BB22_4;

BB22_11:
	shl.b64 	%rd67, %rd91, 3;
	add.s64 	%rd68, %rd3, %rd67;
	ld.global.u64 	%rd28, [%rd68];
	mov.u64 	%rd87, %rd28;

BB22_4:
	mov.u64 	%rd10, %rd87;
	and.b64  	%rd57, %rd10, -8;
	add.s64 	%rd58, %rd2, %rd57;
	ld.global.u64 	%rd12, [%rd58];
	ld.global.u64 	%rd95, [%rd58+8];
	setp.gt.s64	%p4, %rd95, 0;
	setp.eq.s64	%p5, %rd91, 0;
	and.pred  	%p6, %p5, %p4;
	mov.u64 	%rd88, %rd7;
	mov.u64 	%rd90, %rd53;
	@!%p6 bra 	BB22_6;
	bra.uni 	BB22_5;

BB22_5:
	mov.u64 	%rd15, %rd90;
	mov.u64 	%rd14, %rd88;
	st.global.u64 	[%rd14], %rd53;
	add.s64 	%rd16, %rd14, 8;
	add.s64 	%rd17, %rd15, 1;
	setp.lt.s64	%p7, %rd17, %rd95;
	mov.u64 	%rd88, %rd16;
	mov.u64 	%rd90, %rd17;
	@%p7 bra 	BB22_5;

BB22_6:
	setp.lt.s64	%p8, %rd95, 1;
	@%p8 bra 	BB22_9;

	add.s64 	%rd93, %rd8, %rd57;
	mov.u64 	%rd94, 0;
	mov.u64 	%rd92, %rd9;

BB22_8:
	mov.u64 	%rd19, %rd92;
	ld.global.f64 	%fd1, [%rd19];
	ld.global.f64 	%fd2, [%rd93];
	add.f64 	%fd3, %fd2, %fd1;
	st.global.f64 	[%rd19], %fd3;
	add.s64 	%rd93, %rd93, 8;
	add.s64 	%rd23, %rd19, 8;
	add.s64 	%rd94, %rd94, 1;
	setp.lt.s64	%p9, %rd94, %rd95;
	mov.u64 	%rd92, %rd23;
	@%p9 bra 	BB22_8;

BB22_9:
	add.s64 	%rd91, %rd91, 1;
	setp.lt.s64	%p10, %rd91, %rd5;
	mov.u64 	%rd96, %rd12;
	@%p10 bra 	BB22_11;

BB22_10:
	mov.u64 	%rd27, %rd96;
	and.b64  	%rd62, %rd6, -8;
	add.s64 	%rd63, %rd1, %rd62;
	cvta.to.global.u64 	%rd64, %rd44;
	st.global.u64 	[%rd64], %rd51;
	st.global.u64 	[%rd63], %rd27;
	st.global.u64 	[%rd63+8], %rd95;
	bra.uni 	BB22_21;

BB22_12:
	setp.ge.s64	%p11, %rd4, %rd45;
	@%p11 bra 	BB22_21;

	mov.u32 	%r5, %nctaid.x;
	mul.lo.s32 	%r6, %r5, %r1;
	setp.eq.s32	%p12, %r6, 16384;
	@%p12 bra 	BB22_15;

	mov.u64 	%rd69, $str;
	cvta.global.u64 	%rd70, %rd69;
	mov.u64 	%rd71, $str1;
	cvta.global.u64 	%rd72, %rd71;
	mov.u64 	%rd73, __T22;
	cvta.global.u64 	%rd74, %rd73;
	mov.u32 	%r7, 230;
	mov.u64 	%rd75, 1;
	// Callseq Start 2
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.b64	[param0+0], %rd70;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd72;
	.param .b32 param2;
	st.param.b32	[param2+0], %r7;
	.param .b64 param3;
	st.param.b64	[param3+0], %rd74;
	.param .b64 param4;
	st.param.b64	[param4+0], %rd75;
	call.uni 
	__assertfail, 
	(
	param0, 
	param1, 
	param2, 
	param3, 
	param4
	);
	
	//{
	}// Callseq End 2

BB22_15:
	add.s64 	%rd97, %rd4, 16384;
	setp.ge.s64	%p13, %rd97, %rd45;
	@%p13 bra 	BB22_21;

	shl.b64 	%rd76, %rd4, 3;
	add.s64 	%rd77, %rd3, %rd76;
	ld.global.u64 	%rd78, [%rd77];
	and.b64  	%rd79, %rd78, -8;
	add.s64 	%rd80, %rd79, %rd2;
	add.s64 	%rd30, %rd2, 128;
	add.s64 	%rd31, %rd80, 128;

BB22_17:
	shl.b64 	%rd81, %rd97, 3;
	add.s64 	%rd82, %rd3, %rd81;
	ld.global.u64 	%rd33, [%rd82];
	and.b64  	%rd83, %rd33, -8;
	add.s64 	%rd84, %rd83, %rd2;
	ld.global.u64 	%rd34, [%rd84+8];
	setp.lt.s64	%p14, %rd34, 1;
	@%p14 bra 	BB22_20;

	add.s64 	%rd99, %rd30, %rd83;
	mov.u64 	%rd100, 0;
	mov.u64 	%rd98, %rd31;

BB22_19:
	mov.u64 	%rd36, %rd98;
	ld.global.f64 	%fd4, [%rd36];
	ld.global.f64 	%fd5, [%rd99];
	add.f64 	%fd6, %fd5, %fd4;
	st.global.f64 	[%rd36], %fd6;
	add.s64 	%rd99, %rd99, 8;
	add.s64 	%rd40, %rd36, 8;
	add.s64 	%rd100, %rd100, 1;
	setp.lt.s64	%p15, %rd100, %rd34;
	mov.u64 	%rd98, %rd40;
	@%p15 bra 	BB22_19;

BB22_20:
	add.s64 	%rd97, %rd97, 16384;
	setp.lt.s64	%p16, %rd97, %rd45;
	@%p16 bra 	BB22_17;

BB22_21:
	ret;
}

	// .globl	_Z5LRMapPKlPKdS2_PlPdlS2_
.visible .entry _Z5LRMapPKlPKdS2_PlPdlS2_(
	.param .u64 _Z5LRMapPKlPKdS2_PlPdlS2__param_0,
	.param .u64 _Z5LRMapPKlPKdS2_PlPdlS2__param_1,
	.param .u64 _Z5LRMapPKlPKdS2_PlPdlS2__param_2,
	.param .u64 _Z5LRMapPKlPKdS2_PlPdlS2__param_3,
	.param .u64 _Z5LRMapPKlPKdS2_PlPdlS2__param_4,
	.param .u64 _Z5LRMapPKlPKdS2_PlPdlS2__param_5,
	.param .u64 _Z5LRMapPKlPKdS2_PlPdlS2__param_6
)
{
	.reg .pred 	%p<10>;
	.reg .f32 	%f<3>;
	.reg .s32 	%r<30>;
	.reg .f64 	%fd<63>;
	.reg .s64 	%rd<50>;


	ld.param.u64 	%rd17, [_Z5LRMapPKlPKdS2_PlPdlS2__param_0];
	ld.param.u64 	%rd18, [_Z5LRMapPKlPKdS2_PlPdlS2__param_1];
	ld.param.u64 	%rd22, [_Z5LRMapPKlPKdS2_PlPdlS2__param_2];
	ld.param.u64 	%rd19, [_Z5LRMapPKlPKdS2_PlPdlS2__param_3];
	ld.param.u64 	%rd20, [_Z5LRMapPKlPKdS2_PlPdlS2__param_4];
	ld.param.u64 	%rd23, [_Z5LRMapPKlPKdS2_PlPdlS2__param_5];
	ld.param.u64 	%rd21, [_Z5LRMapPKlPKdS2_PlPdlS2__param_6];
	cvta.to.global.u64 	%rd1, %rd22;
	mov.u32 	%r11, %tid.x;
	cvt.u64.u32	%rd24, %r11;
	mov.u32 	%r12, %ctaid.x;
	mov.u32 	%r13, %ntid.x;
	mul.wide.u32 	%rd25, %r13, %r12;
	add.s64 	%rd2, %rd25, %rd24;
	setp.ge.s64	%p1, %rd2, %rd23;
	@%p1 bra 	BB23_14;

	cvta.to.global.u64 	%rd26, %rd17;
	shl.b64 	%rd27, %rd2, 3;
	add.s64 	%rd28, %rd26, %rd27;
	ld.global.u64 	%rd3, [%rd28];
	and.b64  	%rd29, %rd3, -8;
	add.s64 	%rd30, %rd1, %rd29;
	ld.global.u64 	%rd4, [%rd30];
	cvta.to.global.u64 	%rd31, %rd18;
	add.s64 	%rd32, %rd31, %rd27;
	ld.global.f64 	%fd1, [%rd32];
	ld.global.u64 	%rd5, [%rd30+8];
	cvt.u32.u64	%r1, %rd5;
	mov.f64 	%fd60, 0d0000000000000000;
	setp.lt.s32	%p2, %r1, 1;
	@%p2 bra 	BB23_4;

	cvta.to.global.u64 	%rd47, %rd21;
	add.s64 	%rd34, %rd29, %rd1;
	add.s64 	%rd48, %rd34, 128;
	mov.f64 	%fd60, 0d0000000000000000;
	mov.u32 	%r27, 0;

BB23_3:
	ld.global.f64 	%fd16, [%rd48];
	ld.global.f64 	%fd17, [%rd47];
	fma.rn.f64 	%fd60, %fd17, %fd16, %fd60;
	add.s64 	%rd48, %rd48, 8;
	add.s64 	%rd47, %rd47, 8;
	add.s32 	%r27, %r27, 1;
	setp.lt.s32	%p3, %r27, %r1;
	@%p3 bra 	BB23_3;

BB23_4:
	mul.f64 	%fd5, %fd1, %fd60;
	neg.f64 	%fd6, %fd5;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r4}, %fd6;
	}
	mov.b32 	 %f1, %r4;
	abs.ftz.f32 	%f2, %f1;
	setp.lt.ftz.f32	%p4, %f2, 0f40874911;
	@%p4 bra 	BB23_6;
	bra.uni 	BB23_5;

BB23_6:
	mov.f64 	%fd22, 0d3FF71547652B82FE;
	mul.rn.f64 	%fd23, %fd6, %fd22;
	mov.f64 	%fd24, 0d4338000000000000;
	add.rn.f64 	%fd25, %fd23, %fd24;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r5, %temp}, %fd25;
	}
	mov.f64 	%fd26, 0dC338000000000000;
	add.rn.f64 	%fd27, %fd25, %fd26;
	mov.f64 	%fd28, 0dBFE62E42FEFA39EF;
	fma.rn.f64 	%fd29, %fd27, %fd28, %fd6;
	mov.f64 	%fd30, 0dBC7ABC9E3B39803F;
	fma.rn.f64 	%fd31, %fd27, %fd30, %fd29;
	mov.f64 	%fd32, 0d3E928AF3FCA213EA;
	mov.f64 	%fd33, 0d3E5ADE1569CE2BDF;
	fma.rn.f64 	%fd34, %fd33, %fd31, %fd32;
	mov.f64 	%fd35, 0d3EC71DEE62401315;
	fma.rn.f64 	%fd36, %fd34, %fd31, %fd35;
	mov.f64 	%fd37, 0d3EFA01997C89EB71;
	fma.rn.f64 	%fd38, %fd36, %fd31, %fd37;
	mov.f64 	%fd39, 0d3F2A01A014761F65;
	fma.rn.f64 	%fd40, %fd38, %fd31, %fd39;
	mov.f64 	%fd41, 0d3F56C16C1852B7AF;
	fma.rn.f64 	%fd42, %fd40, %fd31, %fd41;
	mov.f64 	%fd43, 0d3F81111111122322;
	fma.rn.f64 	%fd44, %fd42, %fd31, %fd43;
	mov.f64 	%fd45, 0d3FA55555555502A1;
	fma.rn.f64 	%fd46, %fd44, %fd31, %fd45;
	mov.f64 	%fd47, 0d3FC5555555555511;
	fma.rn.f64 	%fd48, %fd46, %fd31, %fd47;
	mov.f64 	%fd49, 0d3FE000000000000B;
	fma.rn.f64 	%fd50, %fd48, %fd31, %fd49;
	mov.f64 	%fd51, 0d3FF0000000000000;
	fma.rn.f64 	%fd52, %fd50, %fd31, %fd51;
	fma.rn.f64 	%fd61, %fd52, %fd31, %fd51;
	abs.s32 	%r15, %r5;
	setp.lt.s32	%p7, %r15, 1023;
	@%p7 bra 	BB23_8;
	bra.uni 	BB23_7;

BB23_8:
	shl.b32 	%r21, %r5, 20;
	add.s32 	%r28, %r21, 1072693248;
	bra.uni 	BB23_9;

BB23_5:
	setp.lt.s32	%p5, %r4, 0;
	selp.f64	%fd18, 0d0000000000000000, 0d7FF0000000000000, %p5;
	abs.f64 	%fd20, %fd6;
	setp.gtu.f64	%p6, %fd20, 0d7FF0000000000000;
	sub.f64 	%fd21, %fd6, %fd5;
	selp.f64	%fd62, %fd21, %fd18, %p6;
	bra.uni 	BB23_10;

BB23_7:
	add.s32 	%r16, %r5, 2046;
	shl.b32 	%r17, %r16, 19;
	and.b32  	%r18, %r17, -1048576;
	shl.b32 	%r19, %r16, 20;
	sub.s32 	%r28, %r19, %r18;
	mov.u32 	%r20, 0;
	mov.b64 	%fd53, {%r20, %r18};
	mul.f64 	%fd61, %fd61, %fd53;

BB23_9:
	mov.u32 	%r22, 0;
	mov.b64 	%fd54, {%r22, %r28};
	mul.f64 	%fd62, %fd61, %fd54;

BB23_10:
	@%p2 bra 	BB23_13;

	cvta.to.global.u64 	%rd12, %rd20;
	add.f64 	%fd55, %fd62, 0d3FF0000000000000;
	rcp.rn.f64 	%fd56, %fd55;
	add.f64 	%fd57, %fd56, 0dBFF0000000000000;
	mul.f64 	%fd13, %fd1, %fd57;
	add.s64 	%rd49, %rd29, 128;
	mov.u32 	%r29, 0;

BB23_12:
	add.s64 	%rd36, %rd1, %rd49;
	ld.global.f64 	%fd58, [%rd36];
	mul.f64 	%fd59, %fd13, %fd58;
	add.s64 	%rd37, %rd12, %rd49;
	st.global.f64 	[%rd37], %fd59;
	add.s64 	%rd49, %rd49, 8;
	add.s32 	%r29, %r29, 1;
	setp.lt.s32	%p9, %r29, %r1;
	@%p9 bra 	BB23_12;

BB23_13:
	cvta.to.global.u64 	%rd38, %rd19;
	add.s64 	%rd43, %rd38, %rd27;
	st.global.u64 	[%rd43], %rd3;
	cvta.to.global.u64 	%rd44, %rd20;
	add.s64 	%rd46, %rd44, %rd29;
	st.global.u64 	[%rd46], %rd4;
	st.global.u64 	[%rd46+8], %rd5;

BB23_14:
	ret;
}

	// .globl	_Z8LRReducePKlPKdPlPdlii
.visible .entry _Z8LRReducePKlPKdPlPdlii(
	.param .u64 _Z8LRReducePKlPKdPlPdlii_param_0,
	.param .u64 _Z8LRReducePKlPKdPlPdlii_param_1,
	.param .u64 _Z8LRReducePKlPKdPlPdlii_param_2,
	.param .u64 _Z8LRReducePKlPKdPlPdlii_param_3,
	.param .u64 _Z8LRReducePKlPKdPlPdlii_param_4,
	.param .u32 _Z8LRReducePKlPKdPlPdlii_param_5,
	.param .u32 _Z8LRReducePKlPKdPlPdlii_param_6
)
{
	.reg .pred 	%p<20>;
	.reg .s32 	%r<392>;
	.reg .f64 	%fd<103>;
	.reg .s64 	%rd<111>;


	ld.param.u64 	%rd39, [_Z8LRReducePKlPKdPlPdlii_param_0];
	ld.param.u64 	%rd40, [_Z8LRReducePKlPKdPlPdlii_param_1];
	ld.param.u64 	%rd41, [_Z8LRReducePKlPKdPlPdlii_param_2];
	ld.param.u64 	%rd42, [_Z8LRReducePKlPKdPlPdlii_param_3];
	ld.param.u64 	%rd43, [_Z8LRReducePKlPKdPlPdlii_param_4];
	ld.param.u32 	%r12, [_Z8LRReducePKlPKdPlPdlii_param_5];
	mov.u32 	%r13, %ctaid.x;
	mov.u32 	%r14, %ntid.x;
	mov.u32 	%r15, %tid.x;
	mad.lo.s32 	%r1, %r13, %r14, %r15;
	cvt.s64.s32	%rd1, %r1;
	setp.lt.s64	%p1, %rd1, %rd43;
	setp.eq.s32	%p2, %r12, 0;
	and.pred  	%p3, %p2, %p1;
	@!%p3 bra 	BB24_27;
	bra.uni 	BB24_1;

BB24_1:
	cvta.to.global.u64 	%rd44, %rd40;
	cvta.to.global.u64 	%rd45, %rd39;
	cvta.to.global.u64 	%rd46, %rd41;
	shl.b64 	%rd47, %rd1, 3;
	add.s64 	%rd48, %rd45, %rd47;
	ld.global.u64 	%rd49, [%rd48];
	and.b64  	%rd50, %rd49, -8;
	add.s64 	%rd51, %rd44, %rd50;
	ld.global.u64 	%rd2, [%rd51];
	ld.global.u64 	%rd3, [%rd51+8];
	mov.u64 	%rd108, 0;
	st.global.u64 	[%rd46], %rd108;
	setp.ge.s64	%p4, %rd1, %rd3;
	@%p4 bra 	BB24_3;

	cvta.to.global.u64 	%rd53, %rd42;
	add.s64 	%rd55, %rd47, %rd53;
	mov.u64 	%rd56, 0;
	st.global.u64 	[%rd55+128], %rd56;

BB24_3:
	setp.lt.s64	%p5, %rd3, 4;
	@%p5 bra 	BB24_18;

	cvt.u32.u64	%r16, %rd1;
	add.s32 	%r17, %r1, 16;
	shr.s32 	%r18, %r17, 31;
	shr.u32 	%r19, %r18, 27;
	add.s32 	%r20, %r17, %r19;
	and.b32  	%r21, %r20, -32;
	sub.s32 	%r2, %r17, %r21;
	add.s32 	%r22, %r1, 8;
	shr.s32 	%r23, %r22, 31;
	shr.u32 	%r24, %r23, 27;
	add.s32 	%r25, %r22, %r24;
	and.b32  	%r26, %r25, -32;
	sub.s32 	%r3, %r22, %r26;
	add.s32 	%r27, %r1, 4;
	shr.s32 	%r28, %r27, 31;
	shr.u32 	%r29, %r28, 27;
	add.s32 	%r30, %r27, %r29;
	and.b32  	%r31, %r30, -32;
	sub.s32 	%r4, %r27, %r31;
	add.s32 	%r32, %r1, 2;
	shr.s32 	%r33, %r32, 31;
	shr.u32 	%r34, %r33, 27;
	add.s32 	%r35, %r32, %r34;
	and.b32  	%r36, %r35, -32;
	sub.s32 	%r5, %r32, %r36;
	add.s32 	%r37, %r16, 1;
	shr.s32 	%r38, %r37, 31;
	shr.u32 	%r39, %r38, 27;
	add.s32 	%r40, %r37, %r39;
	and.b32  	%r41, %r40, -32;
	sub.s32 	%r6, %r37, %r41;
	mov.u64 	%rd108, 0;

BB24_5:
	cvt.u64.u32	%rd103, %r1;
	mov.f64 	%fd99, 0d0000000000000000;
	mov.f64 	%fd98, %fd99;
	mov.f64 	%fd97, %fd99;
	mov.f64 	%fd96, %fd99;
	setp.ge.s64	%p6, %rd103, %rd43;
	@%p6 bra 	BB24_8;

	mov.u32 	%r50, %nctaid.x;
	mul.lo.s32 	%r51, %r50, %r14;
	cvt.u64.u32	%rd8, %r51;
	mov.f64 	%fd96, 0d0000000000000000;
	mov.f64 	%fd97, %fd96;
	mov.f64 	%fd98, %fd96;
	mov.f64 	%fd99, %fd96;

BB24_7:
	shl.b64 	%rd60, %rd103, 3;
	add.s64 	%rd61, %rd45, %rd60;
	ld.global.u64 	%rd62, [%rd61];
	shr.u64 	%rd63, %rd62, 3;
	add.s64 	%rd64, %rd108, %rd63;
	shl.b64 	%rd65, %rd64, 3;
	add.s64 	%rd66, %rd65, %rd44;
	ld.global.f64 	%fd29, [%rd66+128];
	add.f64 	%fd96, %fd96, %fd29;
	ld.global.f64 	%fd30, [%rd66+136];
	add.f64 	%fd97, %fd97, %fd30;
	ld.global.f64 	%fd31, [%rd66+144];
	add.f64 	%fd98, %fd98, %fd31;
	ld.global.f64 	%fd32, [%rd66+152];
	add.f64 	%fd99, %fd99, %fd32;
	add.s64 	%rd103, %rd8, %rd103;
	setp.lt.s64	%p7, %rd103, %rd43;
	@%p7 bra 	BB24_7;

BB24_8:
	and.b32  	%r293, %r15, 31;
	// inline asm
	mov.b64 {%r52,%r53}, %fd96;
	// inline asm
	// inline asm
	mov.b64 {%r54,%r55}, %fd97;
	// inline asm
	// inline asm
	mov.b64 {%r56,%r57}, %fd98;
	// inline asm
	// inline asm
	mov.b64 {%r58,%r59}, %fd99;
	// inline asm
	mov.u32 	%r283, 31;
	// inline asm
	shfl.idx.b32 %r60, %r52, %r2, %r283;
	// inline asm
	// inline asm
	shfl.idx.b32 %r64, %r53, %r2, %r283;
	// inline asm
	// inline asm
	shfl.idx.b32 %r68, %r54, %r2, %r283;
	// inline asm
	// inline asm
	shfl.idx.b32 %r72, %r55, %r2, %r283;
	// inline asm
	// inline asm
	shfl.idx.b32 %r76, %r56, %r2, %r283;
	// inline asm
	// inline asm
	shfl.idx.b32 %r80, %r57, %r2, %r283;
	// inline asm
	// inline asm
	shfl.idx.b32 %r84, %r58, %r2, %r283;
	// inline asm
	// inline asm
	shfl.idx.b32 %r88, %r59, %r2, %r283;
	// inline asm
	// inline asm
	mov.b64 %fd37, {%r60,%r64};
	// inline asm
	// inline asm
	mov.b64 %fd38, {%r68,%r72};
	// inline asm
	// inline asm
	mov.b64 %fd39, {%r76,%r80};
	// inline asm
	// inline asm
	mov.b64 %fd40, {%r84,%r88};
	// inline asm
	add.f64 	%fd41, %fd96, %fd37;
	add.f64 	%fd42, %fd97, %fd38;
	add.f64 	%fd43, %fd98, %fd39;
	add.f64 	%fd44, %fd99, %fd40;
	// inline asm
	mov.b64 {%r100,%r101}, %fd41;
	// inline asm
	// inline asm
	mov.b64 {%r102,%r103}, %fd42;
	// inline asm
	// inline asm
	mov.b64 {%r104,%r105}, %fd43;
	// inline asm
	// inline asm
	mov.b64 {%r106,%r107}, %fd44;
	// inline asm
	// inline asm
	shfl.idx.b32 %r108, %r100, %r3, %r283;
	// inline asm
	// inline asm
	shfl.idx.b32 %r112, %r101, %r3, %r283;
	// inline asm
	// inline asm
	shfl.idx.b32 %r116, %r102, %r3, %r283;
	// inline asm
	// inline asm
	shfl.idx.b32 %r120, %r103, %r3, %r283;
	// inline asm
	// inline asm
	shfl.idx.b32 %r124, %r104, %r3, %r283;
	// inline asm
	// inline asm
	shfl.idx.b32 %r128, %r105, %r3, %r283;
	// inline asm
	// inline asm
	shfl.idx.b32 %r132, %r106, %r3, %r283;
	// inline asm
	// inline asm
	shfl.idx.b32 %r136, %r107, %r3, %r283;
	// inline asm
	// inline asm
	mov.b64 %fd45, {%r108,%r112};
	// inline asm
	// inline asm
	mov.b64 %fd46, {%r116,%r120};
	// inline asm
	// inline asm
	mov.b64 %fd47, {%r124,%r128};
	// inline asm
	// inline asm
	mov.b64 %fd48, {%r132,%r136};
	// inline asm
	add.f64 	%fd49, %fd41, %fd45;
	add.f64 	%fd50, %fd42, %fd46;
	add.f64 	%fd51, %fd43, %fd47;
	add.f64 	%fd52, %fd44, %fd48;
	// inline asm
	mov.b64 {%r148,%r149}, %fd49;
	// inline asm
	// inline asm
	mov.b64 {%r150,%r151}, %fd50;
	// inline asm
	// inline asm
	mov.b64 {%r152,%r153}, %fd51;
	// inline asm
	// inline asm
	mov.b64 {%r154,%r155}, %fd52;
	// inline asm
	// inline asm
	shfl.idx.b32 %r156, %r148, %r4, %r283;
	// inline asm
	// inline asm
	shfl.idx.b32 %r160, %r149, %r4, %r283;
	// inline asm
	// inline asm
	shfl.idx.b32 %r164, %r150, %r4, %r283;
	// inline asm
	// inline asm
	shfl.idx.b32 %r168, %r151, %r4, %r283;
	// inline asm
	// inline asm
	shfl.idx.b32 %r172, %r152, %r4, %r283;
	// inline asm
	// inline asm
	shfl.idx.b32 %r176, %r153, %r4, %r283;
	// inline asm
	// inline asm
	shfl.idx.b32 %r180, %r154, %r4, %r283;
	// inline asm
	// inline asm
	shfl.idx.b32 %r184, %r155, %r4, %r283;
	// inline asm
	// inline asm
	mov.b64 %fd53, {%r156,%r160};
	// inline asm
	// inline asm
	mov.b64 %fd54, {%r164,%r168};
	// inline asm
	// inline asm
	mov.b64 %fd55, {%r172,%r176};
	// inline asm
	// inline asm
	mov.b64 %fd56, {%r180,%r184};
	// inline asm
	add.f64 	%fd57, %fd49, %fd53;
	add.f64 	%fd58, %fd50, %fd54;
	add.f64 	%fd59, %fd51, %fd55;
	add.f64 	%fd60, %fd52, %fd56;
	// inline asm
	mov.b64 {%r196,%r197}, %fd57;
	// inline asm
	// inline asm
	mov.b64 {%r198,%r199}, %fd58;
	// inline asm
	// inline asm
	mov.b64 {%r200,%r201}, %fd59;
	// inline asm
	// inline asm
	mov.b64 {%r202,%r203}, %fd60;
	// inline asm
	// inline asm
	shfl.idx.b32 %r204, %r196, %r5, %r283;
	// inline asm
	// inline asm
	shfl.idx.b32 %r208, %r197, %r5, %r283;
	// inline asm
	// inline asm
	shfl.idx.b32 %r212, %r198, %r5, %r283;
	// inline asm
	// inline asm
	shfl.idx.b32 %r216, %r199, %r5, %r283;
	// inline asm
	// inline asm
	shfl.idx.b32 %r220, %r200, %r5, %r283;
	// inline asm
	// inline asm
	shfl.idx.b32 %r224, %r201, %r5, %r283;
	// inline asm
	// inline asm
	shfl.idx.b32 %r228, %r202, %r5, %r283;
	// inline asm
	// inline asm
	shfl.idx.b32 %r232, %r203, %r5, %r283;
	// inline asm
	// inline asm
	mov.b64 %fd61, {%r204,%r208};
	// inline asm
	// inline asm
	mov.b64 %fd62, {%r212,%r216};
	// inline asm
	// inline asm
	mov.b64 %fd63, {%r220,%r224};
	// inline asm
	// inline asm
	mov.b64 %fd64, {%r228,%r232};
	// inline asm
	add.f64 	%fd65, %fd57, %fd61;
	add.f64 	%fd66, %fd58, %fd62;
	add.f64 	%fd67, %fd59, %fd63;
	add.f64 	%fd68, %fd60, %fd64;
	// inline asm
	mov.b64 {%r244,%r245}, %fd65;
	// inline asm
	// inline asm
	mov.b64 {%r246,%r247}, %fd66;
	// inline asm
	// inline asm
	mov.b64 {%r248,%r249}, %fd67;
	// inline asm
	// inline asm
	mov.b64 {%r250,%r251}, %fd68;
	// inline asm
	// inline asm
	shfl.idx.b32 %r252, %r244, %r6, %r283;
	// inline asm
	// inline asm
	shfl.idx.b32 %r256, %r245, %r6, %r283;
	// inline asm
	// inline asm
	shfl.idx.b32 %r260, %r246, %r6, %r283;
	// inline asm
	// inline asm
	shfl.idx.b32 %r264, %r247, %r6, %r283;
	// inline asm
	// inline asm
	shfl.idx.b32 %r268, %r248, %r6, %r283;
	// inline asm
	// inline asm
	shfl.idx.b32 %r272, %r249, %r6, %r283;
	// inline asm
	// inline asm
	shfl.idx.b32 %r276, %r250, %r6, %r283;
	// inline asm
	// inline asm
	shfl.idx.b32 %r280, %r251, %r6, %r283;
	// inline asm
	// inline asm
	mov.b64 %fd69, {%r252,%r256};
	// inline asm
	// inline asm
	mov.b64 %fd70, {%r260,%r264};
	// inline asm
	// inline asm
	mov.b64 %fd71, {%r268,%r272};
	// inline asm
	// inline asm
	mov.b64 %fd72, {%r276,%r280};
	// inline asm
	add.f64 	%fd13, %fd65, %fd69;
	add.f64 	%fd14, %fd66, %fd70;
	add.f64 	%fd15, %fd67, %fd71;
	add.f64 	%fd16, %fd68, %fd72;
	setp.ne.s32	%p8, %r293, 0;
	@%p8 bra 	BB24_17;

	cvta.to.global.u64 	%rd67, %rd42;
	shl.b64 	%rd68, %rd108, 3;
	add.s64 	%rd69, %rd67, %rd68;
	ld.global.u64 	%rd104, [%rd69+128];

BB24_10:
	mov.u64 	%rd12, %rd104;
	mov.b64 	 %fd73, %rd12;
	add.f64 	%fd74, %fd13, %fd73;
	mov.b64 	 %rd70, %fd74;
	add.s64 	%rd74, %rd69, 128;
	atom.global.cas.b64 	%rd104, [%rd74], %rd12, %rd70;
	setp.ne.s64	%p9, %rd12, %rd104;
	@%p9 bra 	BB24_10;

	add.s64 	%rd14, %rd69, 136;
	ld.global.u64 	%rd105, [%rd69+136];

BB24_12:
	mov.u64 	%rd16, %rd105;
	mov.b64 	 %fd75, %rd16;
	add.f64 	%fd76, %fd14, %fd75;
	mov.b64 	 %rd78, %fd76;
	atom.global.cas.b64 	%rd105, [%rd14], %rd16, %rd78;
	setp.ne.s64	%p10, %rd16, %rd105;
	@%p10 bra 	BB24_12;

	add.s64 	%rd18, %rd69, 144;
	ld.global.u64 	%rd106, [%rd69+144];

BB24_14:
	mov.u64 	%rd20, %rd106;
	mov.b64 	 %fd77, %rd20;
	add.f64 	%fd78, %fd15, %fd77;
	mov.b64 	 %rd82, %fd78;
	atom.global.cas.b64 	%rd106, [%rd18], %rd20, %rd82;
	setp.ne.s64	%p11, %rd20, %rd106;
	@%p11 bra 	BB24_14;

	add.s64 	%rd22, %rd69, 152;
	ld.global.u64 	%rd107, [%rd69+152];

BB24_16:
	mov.u64 	%rd24, %rd107;
	mov.b64 	 %fd79, %rd24;
	add.f64 	%fd80, %fd16, %fd79;
	mov.b64 	 %rd86, %fd80;
	atom.global.cas.b64 	%rd107, [%rd22], %rd24, %rd86;
	setp.ne.s64	%p12, %rd24, %rd107;
	@%p12 bra 	BB24_16;

BB24_17:
	add.s64 	%rd108, %rd108, 4;
	sub.s64 	%rd87, %rd3, %rd108;
	setp.gt.s64	%p13, %rd87, 3;
	@%p13 bra 	BB24_5;

BB24_18:
	setp.ge.s64	%p14, %rd108, %rd3;
	@%p14 bra 	BB24_26;

	add.s32 	%r298, %r1, 16;
	shr.s32 	%r299, %r298, 31;
	shr.u32 	%r300, %r299, 27;
	add.s32 	%r301, %r298, %r300;
	and.b32  	%r302, %r301, -32;
	sub.s32 	%r7, %r298, %r302;
	add.s32 	%r303, %r1, 8;
	shr.s32 	%r304, %r303, 31;
	shr.u32 	%r305, %r304, 27;
	add.s32 	%r306, %r303, %r305;
	and.b32  	%r307, %r306, -32;
	sub.s32 	%r8, %r303, %r307;
	add.s32 	%r308, %r1, 4;
	shr.s32 	%r309, %r308, 31;
	shr.u32 	%r310, %r309, 27;
	add.s32 	%r311, %r308, %r310;
	and.b32  	%r312, %r311, -32;
	sub.s32 	%r9, %r308, %r312;
	add.s32 	%r313, %r1, 2;
	shr.s32 	%r314, %r313, 31;
	shr.u32 	%r315, %r314, 27;
	add.s32 	%r316, %r313, %r315;
	and.b32  	%r317, %r316, -32;
	sub.s32 	%r10, %r313, %r317;
	add.s32 	%r318, %r1, 1;
	shr.s32 	%r319, %r318, 31;
	shr.u32 	%r320, %r319, 27;
	add.s32 	%r321, %r318, %r320;
	and.b32  	%r322, %r321, -32;
	sub.s32 	%r11, %r318, %r322;

BB24_20:
	cvt.u64.u32	%rd109, %r1;
	cvta.to.global.u64 	%rd88, %rd42;
	shl.b64 	%rd89, %rd108, 3;
	add.s64 	%rd90, %rd89, %rd88;
	add.s64 	%rd30, %rd108, 16;
	add.s64 	%rd31, %rd90, 128;
	mov.f64 	%fd101, 0d0000000000000000;
	mov.f64 	%fd102, %fd101;
	setp.ge.s64	%p15, %rd109, %rd43;
	@%p15 bra 	BB24_22;

BB24_21:
	shl.b64 	%rd92, %rd109, 3;
	add.s64 	%rd93, %rd45, %rd92;
	ld.global.u64 	%rd94, [%rd93];
	shr.u64 	%rd95, %rd94, 3;
	add.s64 	%rd96, %rd30, %rd95;
	shl.b64 	%rd98, %rd96, 3;
	add.s64 	%rd99, %rd44, %rd98;
	ld.global.f64 	%fd83, [%rd99];
	add.f64 	%fd102, %fd102, %fd83;
	mov.u32 	%r328, %nctaid.x;
	mul.lo.s32 	%r329, %r328, %r14;
	cvt.u64.u32	%rd100, %r329;
	add.s64 	%rd109, %rd100, %rd109;
	setp.lt.s64	%p16, %rd109, %rd43;
	mov.f64 	%fd101, %fd102;
	@%p16 bra 	BB24_21;

BB24_22:
	and.b32  	%r391, %r15, 31;
	// inline asm
	mov.b64 {%r330,%r331}, %fd101;
	// inline asm
	mov.u32 	%r387, 31;
	// inline asm
	shfl.idx.b32 %r332, %r330, %r7, %r387;
	// inline asm
	// inline asm
	shfl.idx.b32 %r336, %r331, %r7, %r387;
	// inline asm
	// inline asm
	mov.b64 %fd85, {%r332,%r336};
	// inline asm
	add.f64 	%fd86, %fd101, %fd85;
	// inline asm
	mov.b64 {%r342,%r343}, %fd86;
	// inline asm
	// inline asm
	shfl.idx.b32 %r344, %r342, %r8, %r387;
	// inline asm
	// inline asm
	shfl.idx.b32 %r348, %r343, %r8, %r387;
	// inline asm
	// inline asm
	mov.b64 %fd87, {%r344,%r348};
	// inline asm
	add.f64 	%fd88, %fd86, %fd87;
	// inline asm
	mov.b64 {%r354,%r355}, %fd88;
	// inline asm
	// inline asm
	shfl.idx.b32 %r356, %r354, %r9, %r387;
	// inline asm
	// inline asm
	shfl.idx.b32 %r360, %r355, %r9, %r387;
	// inline asm
	// inline asm
	mov.b64 %fd89, {%r356,%r360};
	// inline asm
	add.f64 	%fd90, %fd88, %fd89;
	// inline asm
	mov.b64 {%r366,%r367}, %fd90;
	// inline asm
	// inline asm
	shfl.idx.b32 %r368, %r366, %r10, %r387;
	// inline asm
	// inline asm
	shfl.idx.b32 %r372, %r367, %r10, %r387;
	// inline asm
	// inline asm
	mov.b64 %fd91, {%r368,%r372};
	// inline asm
	add.f64 	%fd92, %fd90, %fd91;
	// inline asm
	mov.b64 {%r378,%r379}, %fd92;
	// inline asm
	// inline asm
	shfl.idx.b32 %r380, %r378, %r11, %r387;
	// inline asm
	// inline asm
	shfl.idx.b32 %r384, %r379, %r11, %r387;
	// inline asm
	// inline asm
	mov.b64 %fd93, {%r380,%r384};
	// inline asm
	add.f64 	%fd20, %fd92, %fd93;
	setp.ne.s32	%p17, %r391, 0;
	@%p17 bra 	BB24_25;

	ld.global.u64 	%rd110, [%rd31];

BB24_24:
	mov.u64 	%rd36, %rd110;
	mov.b64 	 %fd94, %rd36;
	add.f64 	%fd95, %fd20, %fd94;
	mov.b64 	 %rd101, %fd95;
	atom.global.cas.b64 	%rd110, [%rd31], %rd36, %rd101;
	setp.ne.s64	%p18, %rd36, %rd110;
	@%p18 bra 	BB24_24;

BB24_25:
	add.s64 	%rd108, %rd108, 1;
	setp.lt.s64	%p19, %rd108, %rd3;
	@%p19 bra 	BB24_20;

BB24_26:
	cvta.to.global.u64 	%rd102, %rd42;
	st.global.u64 	[%rd102], %rd2;
	st.global.u64 	[%rd102+8], %rd3;

BB24_27:
	ret;
}


